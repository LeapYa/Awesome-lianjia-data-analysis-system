import time
import random
import re
import logging
import datetime
import traceback
import os
import json
import psycopg2
from psycopg2 import pool
from psycopg2 import errors as psycopg2_errors
from urllib.parse import urlparse
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import requests
import threading
import concurrent.futures  # å¯¼å…¥å¹¶è¡Œå¤„ç†æ¨¡å—
import ip_manager  # å¯¼å…¥IPç®¡ç†æ¨¡å—
# å¯¼å…¥æ•°æ®åº“é…ç½®
import db_config
# å¯¼å…¥æ•°æ®åº“å·¥å…·
import db_utils
# å¯¼å…¥æ•°æ®åˆ†ææ¨¡å—
import data_processor

# å¯¼å…¥DrissionPage
from DrissionPage import ChromiumPage
from DrissionPage.errors import ElementNotFoundError
from DrissionPage._configs.chromium_options import ChromiumOptions
import cv2
import numpy as np
# å¯¼å…¥éªŒè¯ç®¡ç†å™¨
import verification_manager

# ç¡®ä¿logsç›®å½•å­˜åœ¨
logs_dir = "logs"
if not os.path.exists(logs_dir):
    os.makedirs(logs_dir)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(logs_dir, "selenium_spider.log"), encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("rental_selenium_spider")

# è°ƒæ•´æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘ä¸å¿…è¦çš„è¾“å‡º
file_handler = logging.FileHandler(os.path.join(logs_dir, "selenium_spider_detail.log"), encoding="utf-8")
file_handler.setLevel(logging.ERROR)
# æ§åˆ¶å°åªæ˜¾ç¤ºé‡è¦ä¿¡æ¯
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
# é‡æ–°é…ç½®loggerå¤„ç†å™¨
logger.handlers = [file_handler, console_handler]

# åˆ›å»ºçˆ¬è™«ä¸“ç”¨çš„æ•°æ®åº“è¿æ¥æ± 
connection_pool = db_config.create_spider_pool()
if not connection_pool:
    logger.error("æ— æ³•åˆ›å»ºçˆ¬è™«æ•°æ®åº“è¿æ¥æ± ï¼Œç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")

# è·å–æ•°æ®åº“è¿æ¥çš„è¾…åŠ©å‡½æ•°
def get_db_connection():
    """ä»è¿æ¥æ± è·å–æ•°æ®åº“è¿æ¥ï¼Œå¹¶ç¡®ä¿ä½¿ç”¨åæ­£ç¡®å½’è¿˜"""
    return db_config.get_connection(connection_pool)

# åˆ›å»ºè£…é¥°å™¨å®ä¾‹ï¼Œä¸“ç”¨äºçˆ¬è™«è¿æ¥æ± 
with_db_connection = db_utils.with_db_connection(connection_pool)

# å­˜å‚¨çº¿ç¨‹IDåˆ°WebDriverçš„æ˜ å°„
_thread_driver_map = {}

# åˆå§‹é»˜è®¤å€¼ï¼Œä¼šåœ¨setup_driverä¸­æ›´æ–°ä¸ºå®é™…å€¼
BROWSER_WINDOW_WIDTH = 1920
BROWSER_WINDOW_HEIGHT = 1080

def register_driver(driver):
    """æ³¨å†Œå½“å‰çº¿ç¨‹çš„WebDriverå¯¹è±¡"""
    thread_id = threading.current_thread().ident
    _thread_driver_map[thread_id] = driver
    logger.info(f"å·²æ³¨å†Œçº¿ç¨‹ {thread_id} çš„WebDriverå¯¹è±¡")
    return thread_id

def get_current_driver(thread_id=None):
    """è·å–å½“å‰çº¿ç¨‹æˆ–æŒ‡å®šçº¿ç¨‹çš„WebDriverå¯¹è±¡"""
    if thread_id is None:
        thread_id = threading.current_thread().ident
    
    driver = _thread_driver_map.get(thread_id)
    if driver:
        logger.info(f"å·²æ‰¾åˆ°çº¿ç¨‹ {thread_id} çš„WebDriverå¯¹è±¡")
    else:
        logger.warning(f"æœªæ‰¾åˆ°çº¿ç¨‹ {thread_id} çš„WebDriverå¯¹è±¡")
    
    return driver

def unregister_driver():
    """æ³¨é”€å½“å‰çº¿ç¨‹çš„WebDriverå¯¹è±¡"""
    thread_id = threading.current_thread().ident
    if thread_id in _thread_driver_map:
        del _thread_driver_map[thread_id]
        logger.info(f"å·²æ³¨é”€çº¿ç¨‹ {thread_id} çš„WebDriverå¯¹è±¡")

def get_house_id_from_url(url):
    """ä»URLä¸­æå–æˆ¿æºID"""
    try:
        if not url:
            logger.warning("å°è¯•ä»ç©ºURLæå–house_id")
            return None
            
        # è®°å½•æ­£åœ¨å¤„ç†çš„URL
        logger.info(f"æ­£åœ¨ä»URLæå–house_id: {url}")
        
        # ä¾‹å¦‚ä» https://nj.zu.ke.com/zufang/ZHANJIANG2032311205850251264.html æå–ID
        match = re.search(r'zufang/(\w+)\.html', url)
        if match:
            house_id = match.group(1)
            logger.info(f"ä»zufangç±»å‹URLæå–åˆ°house_id: {house_id}")
            return house_id
        
        # å¤„ç†å…¬å¯“ç±»å‹URL: https://sh.zu.ke.com/apartment/74943.html
        apartment_match = re.search(r'apartment/(\d+)\.html', url)
        if apartment_match:
            # æ·»åŠ å‰ç¼€ä»¥åŒºåˆ†å…¬å¯“ç±»å‹ID
            house_id = f"APT{apartment_match.group(1)}"
            logger.info(f"ä»apartmentç±»å‹URLæå–åˆ°house_id: {house_id}")
            return house_id
            
        # å¤„ç†å…¶ä»–å¯èƒ½çš„æ ¼å¼: ä¾‹å¦‚https://sh.zu.ke.com/details/12345.html
        details_match = re.search(r'details/(\w+)\.html', url)
        if details_match:
            house_id = f"DTL{details_match.group(1)}"
            logger.info(f"ä»detailsç±»å‹URLæå–åˆ°house_id: {house_id}")
            return house_id
            
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ ¼å¼ï¼Œä»æ•´ä¸ªURLç”Ÿæˆä¸€ä¸ªå”¯ä¸€ID
        logger.warning(f"æœªèƒ½åŒ¹é…URLæ ¼å¼: {url}ï¼Œå°†ç”Ÿæˆå“ˆå¸ŒID")
        import hashlib
        hash_id = "URL" + hashlib.md5(url.encode()).hexdigest()[:20]
        logger.info(f"ä¸ºæœªçŸ¥æ ¼å¼URLç”Ÿæˆå“ˆå¸ŒID: {hash_id}")
        return hash_id
    except Exception as e:
        logger.error(f"ä»URLæå–house_idæ—¶å‡ºé”™: {str(e)}ï¼ŒURL: {url}")
        # å‡ºé”™æ—¶ä¹Ÿç”Ÿæˆä¸€ä¸ªåŸºäºURLçš„å”¯ä¸€ID
        import hashlib
        return "ERR" + hashlib.md5(url.encode()).hexdigest()[:20]

def parse_room_info(room_text):
    """è§£ææˆ·å‹ä¿¡æ¯ï¼Œæå–æˆ¿é—´æ•°ã€å…æ•°å’Œå«ç”Ÿé—´æ•°"""
    try:
        room_count = hall_count = bath_count = 0
        room_match = re.search(r'(\d+)å®¤', room_text)
        hall_match = re.search(r'(\d+)å…', room_text)
        bath_match = re.search(r'(\d+)å«', room_text)
        
        if room_match:
            room_count = int(room_match.group(1))
        if hall_match:
            hall_count = int(hall_match.group(1))
        if bath_match:
            bath_count = int(bath_match.group(1))
            
        return room_count, hall_count, bath_count
    except:
        return 0, 0, 0

def parse_layout_to_components(layout_str):
    """
    è§£ææˆ·å‹å¸ƒå±€å­—ç¬¦ä¸²ï¼ˆå¦‚"2å®¤1å…1å«"ã€"å¼€é—´"ç­‰ï¼‰ä¸ºç»„ä»¶
    
    Returns:
        tuple: (room_description, room_count, hall_count, bath_count)
    """
    if not layout_str:
        return "", 0, 0, 0
        
    layout_str = layout_str.strip()
    
    # ç‰¹æ®Šæƒ…å†µï¼šå¼€é—´é€šå¸¸æ˜¯1å®¤0å…1å«
    if layout_str == "å¼€é—´":
        return "å¼€é—´", 1, 0, 1
        
    # ä½¿ç”¨å·²æœ‰å‡½æ•°è§£æ
    room_count, hall_count, bath_count = parse_room_info(layout_str)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å«ç”Ÿé—´æ•°é‡ä½†æœ‰å§å®¤ï¼Œé»˜è®¤ä¸º1å«
    if bath_count == 0 and room_count > 0:
        bath_count = 1
        
    return layout_str, room_count, hall_count, bath_count

def start_crawl_task(city, city_code):
    """å¼€å§‹ä¸€ä¸ªæ–°çš„çˆ¬è™«ä»»åŠ¡"""
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºçˆ¬è™«ä»»åŠ¡")
        return None
    
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        # æ’å…¥æ–°ä»»åŠ¡è®°å½•
        cursor.execute(
            "INSERT INTO crawl_task (city, city_code, start_time, status) VALUES (%s, %s, %s, %s) RETURNING id",
            (city, city_code, datetime.datetime.now(), "In Progress")
        )
        task_id = cursor.fetchone()[0]
        conn.commit()
        logger.info(f"åˆ›å»ºçˆ¬è™«ä»»åŠ¡æˆåŠŸï¼Œä»»åŠ¡ID: {task_id}")
        return task_id
    except Exception as e:
        logger.error(f"åˆ›å»ºçˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}")
        return None
    finally:
        if conn:
            connection_pool.putconn(conn)

@with_db_connection
def update_crawl_task(conn, task_id, status, success_items=None, success_pages=None, 
                   failed_pages=None, end_time=None, error=None, total_pages=None, expected_items=None):
    """
    æ›´æ–°çˆ¬è™«ä»»åŠ¡çŠ¶æ€
    """
    try:
        cursor = conn.cursor()
        
        set_clauses = ["status = %s"]
        params = [status]
        
        # æ·»åŠ å¯é€‰å‚æ•°åˆ°æ›´æ–°è¯­å¥ä¸­
        if success_items is not None:
            set_clauses.append("success_items = %s")
            params.append(success_items)
        
        if success_pages is not None:
            set_clauses.append("success_pages = %s")
            params.append(success_pages)
            
        if failed_pages is not None:
            set_clauses.append("failed_pages = %s")
            params.append(failed_pages)
        
        if total_pages is not None:
            set_clauses.append("total_pages = %s")
            params.append(total_pages)
            
        if expected_items is not None:
            set_clauses.append("total_items = %s")
            params.append(expected_items)
            
        if end_time is not None:
            set_clauses.append("end_time = %s")
            params.append(end_time)
            
        if error is not None:
            set_clauses.append("error_message = %s")
            params.append(error)
            
        # æ„å»ºSQLæ›´æ–°è¯­å¥
        query = f"UPDATE crawl_task SET {', '.join(set_clauses)} WHERE id = %s"
        params.append(task_id)
        
        # æ‰§è¡Œæ›´æ–°
        cursor.execute(query, params)
        conn.commit()
        
        logger.info(f"æ›´æ–°çˆ¬è™«ä»»åŠ¡æˆåŠŸï¼Œä»»åŠ¡ID: {task_id}")
        return True
    except Exception as e:
        conn.rollback()
        logger.error(f"æ›´æ–°çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}")
        return False

@with_db_connection
def save_house_info(conn, house_info):
    """
    ä¿å­˜æˆ¿æºä¿¡æ¯åˆ°æ•°æ®åº“ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶
    æ•°æ®åº“åœ¨house_infoå­—å…¸ä¸­å¯¹åº”çš„å­—æ®µå
    id: ID/ä¸»é”®
    house_id -> house_id: æˆ¿æºID
    task_id -> task_id: ä»»åŠ¡ID
    title -> title: æˆ¿æºæ ‡é¢˜
    price -> price: ä»·æ ¼
    district -> location_qu: ä½ç½®ï¼ˆåŒºï¼‰
    community -> location_big: ä½ç½®ï¼ˆå¤§åŒºåŸŸ/å•†åœˆï¼‰
    # location_small: ä½ç½®ï¼ˆå°åŒºåŸŸ/è¡—é“ï¼‰
    area -> size: é¢ç§¯/å¤§å°
    direction -> direction: æœå‘
    features -> room: æˆ¿å‹æè¿°
    floor -> floor: æ¥¼å±‚
    image_url -> image: å›¾ç‰‡
    url -> link: é“¾æ¥
    # unit_price: å•ä½ä»·æ ¼/æ¯å¹³ç±³ä»·æ ¼
    room_count: å§å®¤æ•°é‡(éœ€è¦ä»layoutä¸­è§£æ)
    hall_count: å®¢å…æ•°é‡(éœ€è¦ä»layoutä¸­è§£æ)
    bath_count: å«ç”Ÿé—´æ•°é‡(éœ€è¦ä»layoutä¸­è§£æ)
    layout -> layout: æˆ·å‹å¸ƒå±€
    city_code -> city_code: åŸå¸‚ä»£ç 
    publish_date -> publish_date: å‘å¸ƒæ—¥æœŸ
    features -> features: ç‰¹è‰²æ ‡ç­¾
    created_at -> created_at: åˆ›å»ºæ—¶é—´
    last_updated -> last_updated: æœ€åæ›´æ–°æ—¶é—´
    """
    max_retries = 3
    retry_count = 0
    retry_delay = 2  # åˆå§‹å»¶è¿Ÿ2ç§’
    
    while retry_count < max_retries:
        try:
            cursor = conn.cursor()
            
            # ç¡®ä¿æœ‰house_idï¼Œä»URLæå–
            if 'house_id' not in house_info or not house_info['house_id']:
                house_info['house_id'] = get_house_id_from_url(house_info['url'])
                
                # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•ç”ŸæˆID
                if not house_info['house_id']:
                    logger.warning(f"æ— æ³•ä»URLæå–house_id: {house_info['url']}")
                    import hashlib
                    house_info['house_id'] = "HASH" + hashlib.md5(house_info['url'].encode()).hexdigest()[:20]
                    logger.info(f"ç”Ÿæˆå¤‡ç”¨house_id: {house_info['house_id']}")
            
            # è§£ææˆ·å‹å­—æ®µï¼Œæå–æˆ¿é—´ã€å…å’Œå«ç”Ÿé—´æ•°é‡
            layout_str = house_info.get('layout', '')
            room_description, room_count, hall_count, bath_count = parse_layout_to_components(layout_str)
            
            # è®¡ç®—å•ä»·
            unit_price = 0
            try:
                price = float(house_info.get('price', 0))
                area = float(house_info.get('area', 0))
                if price > 0 and area > 0:
                    unit_price = round(price / area, 2)
            except (ValueError, TypeError):
                logger.warning(f"è®¡ç®—å•ä»·å¤±è´¥: price={house_info.get('price')}, area={house_info.get('area')}")
            
            logger.info(f"å‡†å¤‡ä¿å­˜æˆ¿æºï¼Œhouse_id: {house_info.get('house_id', 'ç©º')}, URL: {house_info.get('url', 'ç©º')}")
            
            # è®°å½•æ‰€æœ‰å°†è¦ä¿å­˜çš„å­—æ®µ
            log_fields = {k: v for k, v in house_info.items() if k != 'features'}
            logger.info(f"æˆ¿æºä¿¡æ¯å­—æ®µ: {log_fields}")
            
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„house_id
            cursor.execute(
                "SELECT id FROM house_info WHERE house_id = %s",
                (house_info['house_id'],)
            )
            existing_by_id = cursor.fetchone()
            
            if existing_by_id:
                logger.info(f"æ›´æ–°å·²å­˜åœ¨çš„æˆ¿æº(æ ¹æ®house_id): {existing_by_id[0]}")
                # å¦‚æœhouse_idå­˜åœ¨ï¼Œåˆ™æ›´æ–°
                try:
                    cursor.execute("""
                    UPDATE house_info SET
                        title = %s,
                        price = %s,
                        layout = %s,
                        size = %s,
                        floor = %s,
                        direction = %s,
                        subway = %s,
                        location_qu = %s,
                        location_big = %s,
                        publish_date = %s,
                        features = %s,
                        image = %s,
                        link = %s,
                        city_code = %s,
                        room = %s,
                        room_count = %s,
                        hall_count = %s,
                        bath_count = %s,
                        unit_price = %s,
                        last_updated = %s
                    WHERE house_id = %s
                    """, (
                        house_info['title'],
                        house_info['price'],
                        house_info['layout'],
                        house_info['area'],
                        house_info['floor'],
                        house_info['direction'],
                        house_info['subway'],
                        house_info['district'],
                        house_info['community'],
                        house_info['publish_date'],
                        json.dumps(house_info['features'], ensure_ascii=False),
                        house_info.get('image_url', ''),
                        house_info['url'],
                        house_info['city_code'],
                        room_description,
                        room_count,
                        hall_count,
                        bath_count,
                        unit_price,
                        datetime.datetime.now(),
                        house_info['house_id']
                    ))
                except Exception as update_err:
                    logger.error(f"æ‰§è¡ŒUPDATEè¯­å¥å¤±è´¥: {str(update_err)}")
                    logger.error(f"UPDATEå‚æ•°: {house_info}")
                    raise update_err
            else:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„URLå’ŒåŸå¸‚ä»£ç 
                cursor.execute(
                    "SELECT id FROM house_info WHERE link = %s AND city_code = %s",
                    (house_info['url'], house_info['city_code'])
                )
                existing_by_url = cursor.fetchone()
                
                if existing_by_url:
                    logger.info(f"æ›´æ–°å·²å­˜åœ¨çš„æˆ¿æº(æ ¹æ®URL): {existing_by_url[0]}")
                    # å¦‚æœURLå’ŒåŸå¸‚ä»£ç å­˜åœ¨ï¼Œåˆ™æ›´æ–°
                    try:
                        cursor.execute("""
                        UPDATE house_info SET
                            title = %s,
                            price = %s,
                            layout = %s,
                            size = %s,
                            floor = %s,
                            direction = %s,
                            subway = %s,
                            location_qu = %s,
                            location_big = %s,
                            publish_date = %s,
                            features = %s,
                            image = %s,
                            house_id = %s,
                            room = %s,
                            room_count = %s,
                            hall_count = %s,
                            bath_count = %s,
                            unit_price = %s,
                            last_updated = %s
                        WHERE link = %s AND city_code = %s
                        """, (
                            house_info['title'],
                            house_info['price'],
                            house_info['layout'],
                            house_info['area'],
                            house_info['floor'],
                            house_info['direction'],
                            house_info['subway'],
                            house_info['district'],
                            house_info['community'],
                            house_info['publish_date'],
                            json.dumps(house_info['features'], ensure_ascii=False),
                            house_info.get('image_url', ''),
                            house_info['house_id'],
                            room_description,
                            room_count,
                            hall_count,
                            bath_count,
                            unit_price,
                            datetime.datetime.now(),
                            house_info['url'],
                            house_info['city_code']
                        ))
                    except Exception as update_err:
                        logger.error(f"æ‰§è¡ŒUPDATEè¯­å¥å¤±è´¥: {str(update_err)}")
                        logger.error(f"UPDATEå‚æ•°: {house_info}")
                        raise update_err
                else:
                    logger.info(f"æ’å…¥æ–°æˆ¿æº: {house_info['house_id']}")
                    # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æ’å…¥
                    try:
                        cursor.execute("""
                        INSERT INTO house_info (
                            link, title, price, layout, size, floor, direction,
                            subway, location_qu, location_big, city_code, publish_date, 
                            features, image, created_at, last_updated, task_id, house_id,
                            room, room_count, hall_count, bath_count, unit_price
                        ) VALUES (
                            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                            %s, %s, %s, %s, %s
                        )
                        """, (
                            house_info['url'],
                            house_info['title'],
                            house_info['price'],
                            house_info['layout'],
                            house_info['area'],
                            house_info['floor'],
                            house_info['direction'],
                            house_info['subway'],
                            house_info['district'],
                            house_info['community'],
                            house_info['city_code'],
                            house_info['publish_date'],
                            json.dumps(house_info['features'], ensure_ascii=False),
                            house_info.get('image_url', ''),
                            datetime.datetime.now(),
                            datetime.datetime.now(),
                            house_info.get('task_id'),
                            house_info['house_id'],
                            room_description,
                            room_count,
                            hall_count,
                            bath_count,
                            unit_price
                        ))
                    except Exception as insert_err:
                        logger.error(f"æ‰§è¡ŒINSERTè¯­å¥å¤±è´¥: {str(insert_err)}")
                        logger.error(f"INSERTå‚æ•°: {house_info}")
                        raise insert_err
            
            conn.commit()
            logger.info(f"ä¿å­˜æˆ¿æºæˆåŠŸ: {house_info['house_id']}")
            return True
            
        except psycopg2.pool.PoolError as e:
            # è¿æ¥æ± æ»¡é”™è¯¯ï¼Œè¿›è¡Œé‡è¯•
            retry_count += 1
            logger.warning(f"ä¿å­˜æˆ¿æºæ•°æ®æ—¶è¿æ¥æ± æ»¡ï¼Œæ­£åœ¨è¿›è¡Œç¬¬{retry_count}æ¬¡é‡è¯•. é”™è¯¯: {str(e)}")
            
            if retry_count < max_retries:
                # éšæœºå¢åŠ å»¶è¿Ÿï¼Œé¿å…æ‰€æœ‰å¤±è´¥è¯·æ±‚åŒæ—¶é‡è¯•
                time.sleep(retry_delay + random.uniform(0, 2))
                retry_delay *= 2  # æŒ‡æ•°é€€é¿
            else:
                logger.error(f"ä¿å­˜æˆ¿æºä¿¡æ¯å¤±è´¥ï¼Œå·²è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")
                return False
                
        except Exception as e:
            # å…¶ä»–é”™è¯¯
            conn.rollback()
            logger.error(f"ä¿å­˜æˆ¿æºä¿¡æ¯å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
            
    return False  # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥

def get_city_url(city_code, page=1):
    """è·å–åŸå¸‚ç§Ÿæˆ¿URL"""
    return f'https://{city_code}.zu.ke.com/zufang/pg{page}'

def is_captcha_page(driver):
    """æ£€æµ‹æ˜¯å¦æ˜¯éªŒè¯ç é¡µé¢"""
    try:
        # æ£€æŸ¥é¡µé¢ä¸­æ˜¯å¦åŒ…å«éªŒè¯ç ç›¸å…³å…ƒç´ 
        try:
            # DrissionPageçš„æŸ¥æ‰¾å…ƒç´ æ–¹æ³•
            captcha_elements = driver.eles('xpath://*[contains(text(), "éªŒè¯ç ") or contains(text(), "äººæœºéªŒè¯") or contains(text(), "å®‰å…¨éªŒè¯") or contains(@id, "captcha")]')
            if captcha_elements:
                logger.warning("æ£€æµ‹åˆ°éªŒè¯ç é¡µé¢")
                return True
        except ElementNotFoundError:
            pass
            
        # æ£€æŸ¥é¡µé¢æ ‡é¢˜æ˜¯å¦åŒ…å«éªŒè¯ç›¸å…³å­—æ ·
        title = driver.title
        if "éªŒè¯" in title or "captcha" in title.lower():
            logger.warning(f"æ£€æµ‹åˆ°éªŒè¯é¡µé¢ï¼Œæ ‡é¢˜: {title}")
            return True
            
        return False
    except Exception as e:
        logger.error(f"æ£€æŸ¥éªŒè¯ç é¡µé¢æ—¶å‡ºé”™: {str(e)}")
        return False

# ä½¿ç”¨ç›´æ¥åƒç´ å·®åˆ†æ³•è¯†åˆ«æ»‘å—ç¼ºå£
def detect_gap_position(bg_bytes):
    """ä½¿ç”¨ç›´æ¥åƒç´ å·®åˆ†æ³•è¯†åˆ«æ»‘å—ç¼ºå£ä½ç½®"""
    try:
        # å°†äºŒè¿›åˆ¶å›¾åƒè½¬æ¢ä¸ºOpenCVæ ¼å¼
        bg_img = cv2.imdecode(np.frombuffer(bg_bytes, np.uint8), cv2.IMREAD_COLOR)
        
        # ä¿å­˜åŸå§‹å›¾ç‰‡ç”¨äºè°ƒè¯•
        cv2.imwrite('debug_original.png', bg_img)
        
        # è·å–å›¾åƒå°ºå¯¸
        height, width = bg_img.shape[:2]
        
        # 1. å°†å›¾åƒè½¬ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(bg_img, cv2.COLOR_BGR2GRAY)
        
        # 2. å¯¹å›¾åƒè¿›è¡Œå¹³æ»‘å¤„ç†
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # 3. ç›´æ¥è®¡ç®—æ¯ä¸€åˆ—çš„åƒç´ å˜åŒ–ç‡
        diff_cols = []
        # æœç´¢èŒƒå›´é™åˆ¶ - ç¼ºå£é€šå¸¸åœ¨ä¸­é—´åå³ä½ç½®ï¼Œéå¸¸é è¾¹çš„ä½ç½®ä¸å¤ªå¯èƒ½
        search_start = int(width * 0.2)  # ä»20%å¤„å¼€å§‹æœç´¢
        search_end = int(width * 0.8)    # æœç´¢åˆ°80%
        
        # è¿™é‡Œæˆ‘ä»¬è®¡ç®—æ¯ä¸€åˆ—çš„ç›¸é‚»åƒç´ å·®åˆ†æ€»å’Œï¼Œçªå˜å¤„å·®åˆ†å€¼è¾ƒå¤§
        for i in range(search_start, search_end):
            # è®¡ç®—åˆ—å†…ç›¸é‚»åƒç´ çš„å·®å¼‚
            col = blur[:, i]
            next_col = blur[:, i+1]
            # è®¡ç®—å·®åˆ†ç»å¯¹å€¼çš„å’Œ
            diff = np.sum(np.abs(col.astype(float) - next_col.astype(float)))
            diff_cols.append((i, diff))
        
        # åˆ¶ä½œå·®åˆ†å¯è§†åŒ–å›¾åƒ
        diff_img = bg_img.copy()
        diff_vals = [d[1] for d in diff_cols]
        max_diff = max(diff_vals) if diff_vals else 1
        
        for i, diff_val in enumerate(diff_vals):
            normalized = int((diff_val / max_diff) * 200)  # å½’ä¸€åŒ–åˆ°0-200çš„èŒƒå›´ï¼Œä½œä¸ºçº¿çš„é«˜åº¦
            x_pos = diff_cols[i][0]
            # ç»˜åˆ¶çº¿æ¡ï¼Œå·®å¼‚è¶Šå¤§ï¼Œçº¿æ¡è¶Šé«˜
            cv2.line(diff_img, (x_pos, height), (x_pos, height - normalized), (0, 0, 255), 1)
        
        # # ä¿å­˜å·®åˆ†å¯è§†åŒ–å›¾åƒ
        # cv2.imwrite('debug_diff.png', diff_img)
        
        # 4. æ‰¾å‡ºå·®å¼‚æœ€å¤§çš„å‡ ä¸ªç‚¹
        # å¯¹å·®åˆ†å€¼è¿›è¡Œæ’åº
        sorted_diffs = sorted(diff_cols, key=lambda x: x[1], reverse=True)
        
        # å–å‰10ä¸ªç‚¹ï¼Œè¿™äº›ç‚¹å¾ˆå¯èƒ½æ˜¯ç¼ºå£ä½ç½®
        top_points = sorted_diffs[:10]
        
        # 5. å¯¹è¿™äº›ç‚¹è¿›è¡Œèšç±»ï¼Œè¿‡æ»¤æ‰å­¤ç«‹ç‚¹
        # é¦–å…ˆæŒ‰xåæ ‡æ’åº
        top_points.sort(key=lambda x: x[0])
        
        # æ‰¾åˆ°ç›¸é‚»ç‚¹çš„ç°‡
        clusters = []
        current_cluster = [top_points[0]]
        
        for i in range(1, len(top_points)):
            # å¦‚æœå½“å‰ç‚¹ä¸ä¸Šä¸€ç‚¹è·ç¦»è¾ƒè¿‘ï¼ŒåŠ å…¥åŒä¸€ç°‡
            if top_points[i][0] - top_points[i-1][0] < 10:  # 10åƒç´ å†…è®¤ä¸ºæ˜¯åŒä¸€ç°‡
                current_cluster.append(top_points[i])
            else:
                # å¦åˆ™å¼€å§‹æ–°çš„ç°‡
                clusters.append(current_cluster)
                current_cluster = [top_points[i]]
        
        # æ·»åŠ æœ€åä¸€ä¸ªç°‡
        if current_cluster:
            clusters.append(current_cluster)
        
        # 6. é€‰å–æœ€å¤§çš„ç°‡
        best_cluster = max(clusters, key=lambda c: sum(p[1] for p in c)) if clusters else []
        
        if best_cluster:
            # è®¡ç®—ç°‡çš„ä¸­å¿ƒç‚¹ä½ç½®
            cluster_x = int(sum(p[0] for p in best_cluster) / len(best_cluster))
            
            # 7. åˆ›å»ºæœ€ç»ˆå¯è§†åŒ–ç»“æœ
            result_img = bg_img.copy()
            
            # åœ¨æ‰€æœ‰å¯èƒ½çš„ç‚¹ä¸Šç”»ä¸Šçº¢ç‚¹
            for x, _ in top_points:
                cv2.circle(result_img, (x, height//2), 3, (0, 0, 255), -1)
            
            # åœ¨é€‰ä¸­çš„ç°‡ä¸­å¿ƒç‚¹ç”»ä¸€æ¡å‚ç›´çº¿
            cv2.line(result_img, (cluster_x, 0), (cluster_x, height), (0, 0, 255), 2)
            
            # ä¸å†ç¼©æ”¾è·ç¦»ï¼Œç›´æ¥ä½¿ç”¨æ£€æµ‹åˆ°çš„åŸå§‹ä½ç½®
            slide_distance = cluster_x
            
            # æ·»åŠ ä½ç½®ä¿¡æ¯æ–‡æœ¬
            font = cv2.FONT_HERSHEY_SIMPLEX
            cv2.putText(result_img, f"Detected: {cluster_x}px", (10, 30), font, 0.7, (0, 0, 255), 2)
            cv2.putText(result_img, f"Slide: {slide_distance}px", (10, 60), font, 0.7, (0, 0, 255), 2)
            
            # ä¿å­˜æœ€ç»ˆç»“æœå›¾åƒ
            cv2.imwrite('debug_final_detection.png', result_img)
            
            logger.info(f"åƒç´ å·®åˆ†æ³•æ£€æµ‹åˆ°ç¼ºå£ä½ç½®: x={cluster_x}")
            
            return slide_distance
        else:
            logger.warning("æœªèƒ½æ£€æµ‹åˆ°æ˜æ˜¾çš„ç¼ºå£")
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾ç¼ºå£ï¼Œæä¾›ä¸€ä¸ªä¸­ç­‰é»˜è®¤å€¼
            return 80
    
    except Exception as e:
        logger.error(f"ç¼ºå£è¯†åˆ«å¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        # ä½¿ç”¨å›ºå®šé»˜è®¤è·ç¦»ï¼Œè€Œä¸æ˜¯åŠ¨æ€è°ƒæ•´
        dis = 80
        logger.info(f"ä½¿ç”¨é»˜è®¤è·ç¦»: {dis}px")
        return dis

# å¤„ç†æ»‘å—éªŒè¯ç 
def solve_slider_captcha(page, session_id=None):
    """
    ä½¿ç”¨DrissionPageè§£å†³æ»‘å—éªŒè¯ç ï¼Œæ— é™å°è¯•ç›´åˆ°æˆåŠŸ
    
    Args:
        page: DrissionPageå®ä¾‹
        session_id: ä¼šè¯IDï¼Œç”¨äºè®°å½•æ—¥å¿—
        
    Returns:
        Dict: å¤„ç†ç»“æœ
    """
    attempt = 0
    logger.info(f"å¼€å§‹å¤„ç†æ»‘å—éªŒè¯ç  [å°è¯• {attempt}], ä¼šè¯ID: {session_id}")
    
    # 1. ç‚¹å‡»å¼€å§‹éªŒè¯æŒ‰é’®
    try:
        # æŸ¥æ‰¾å¼€å§‹éªŒè¯æŒ‰é’®
        verify_btns = page.eles('xpath://div/div[@aria-label]')
        if verify_btns:
            logger.info(f"æ‰¾åˆ°å¼€å§‹éªŒè¯æŒ‰é’®ï¼Œå‡†å¤‡ç‚¹å‡»")
            verify_btns[0].click()
        else:
            logger.info("æœªæ‰¾åˆ°å¼€å§‹éªŒè¯æŒ‰é’®ï¼Œå°è¯•ç›´æ¥æ“ä½œå½“å‰éªŒè¯å†…å®¹")
    except Exception as e:
        logger.warning(f"ç‚¹å‡»å¼€å§‹éªŒè¯æŒ‰é’®å¤±è´¥: {str(e)}")
        
    # 2. ä¼‘çœ ç­‰å¾…éªŒè¯ç åŠ è½½
    time.sleep(2)
    while True:  # æ— é™å¾ªç¯å°è¯•
        attempt += 1
        # 3. è·å–æ»‘å—éªŒè¯ç æœ‰ç¼ºå£çš„å›¾ç‰‡
        logger.info("è·å–èƒŒæ™¯å›¾ç‰‡")
        bg_src = page.run_js('return window.getComputedStyle(document.getElementsByClassName("geetest_bg")[0]).backgroundImage.slice(5, -2)')
        response = requests.get(bg_src)
        response.raise_for_status()
        bg_bytes = response.content  # ä¿å­˜åŸå§‹å­—èŠ‚ç”¨äºå¤„ç†
        
        if not bg_src:
            logger.error("æ— æ³•è·å–èƒŒæ™¯å›¾ç‰‡")
            continue  # å°è¯•ä¸‹ä¸€æ¬¡
        
        # 4. è·å–è¦æ»‘åŠ¨çš„å›¾ç‰‡ (ä»…è®°å½•ï¼Œä¸ä½¿ç”¨)
        logger.info("è·å–æ»‘å—å›¾ç‰‡")
        full_src = page.run_js('return window.getComputedStyle(document.getElementsByClassName("geetest_slice_bg")[0]).backgroundImage.slice(5, -2)')
        
        # 5. ç¼ºå£è¯†åˆ« - ä½¿ç”¨åƒç´ å·®åˆ†æ³•
        logger.info("ä½¿ç”¨åƒç´ å·®åˆ†æ³•è¯†åˆ«ç¼ºå£ä½ç½®")
        try:
            # ä½¿ç”¨åƒç´ å·®åˆ†æ³•è¯†åˆ«
            dis = detect_gap_position(bg_bytes)
            logger.info(f"åƒç´ å·®åˆ†æ³•è¯†åˆ«çš„ç¼ºå£ä½ç½®: x={dis}")
            
            # ä¸å†é™åˆ¶è·ç¦»èŒƒå›´
                
        except Exception as e:
            logger.error(f"ç¼ºå£è¯†åˆ«å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            # ä½¿ç”¨é»˜è®¤è·ç¦»
            dis = 80
            logger.info(f"ä½¿ç”¨é»˜è®¤è·ç¦»: {dis}px")
        
        # 6. æ‹–åŠ¨æ»‘å—
        logger.info(f"å¼€å§‹æ‹–åŠ¨æ»‘å—ï¼Œè·ç¦»: {dis}px")
        try:
            # æŸ¥æ‰¾æ»‘å—æŒ‰é’®
            slide_btn = page.ele('xpath://div[contains(@class,"geetest_track")]/div[contains(@class, "geetest_btn")]')
            
            # å¦‚æœæœªæ‰¾åˆ°æ»‘å—æŒ‰é’®ï¼Œå°è¯•æŸ¥æ‰¾æ»‘å—å…ƒç´ 
            if not slide_btn:
                logger.warning("æœªæ‰¾åˆ°æ»‘å—æŒ‰é’®ï¼Œå°è¯•æŸ¥æ‰¾æ»‘å—å…ƒç´ ")
                slide_btn = page.ele_at_point(20, 200)  # ä¼°è®¡ä½ç½®
                if slide_btn:
                    logger.info(f"åœ¨ä¼°è®¡ä½ç½®æ‰¾åˆ°å…ƒç´ : {slide_btn.tag}")
            
            if not slide_btn:
                logger.error("æ— æ³•æ‰¾åˆ°æ»‘å—æŒ‰é’®")
                continue  # å°è¯•ä¸‹ä¸€æ¬¡
            
            # è·å–åŠ¨ä½œé“¾å¯¹è±¡
            actions = page.actions
            
            # ä½¿ç”¨ç®€å•å¹³æ»‘çš„æ»‘åŠ¨æ–¹å¼
            # é¼ æ ‡ç§»åŠ¨åˆ°æ»‘å—æŒ‰é’®
            actions.move_to(slide_btn)
            
            # æŒ‰ä¸‹é¼ æ ‡å·¦é”®
            actions.m_hold(slide_btn)
            time.sleep(0.2)
            
            # ç¼“æ…¢å¹³æ»‘ç§»åŠ¨
            actions.move(dis - 10, 0)
            time.sleep(0.8)
            actions.move(10, 0)
            time.sleep(1.4)
            actions.move(-10, 0)
            time.sleep(0.5)
            try:
                # æ¾å¼€é¼ æ ‡å·¦é”®
                actions.m_release(slide_btn)
            except Exception as e:
                logger.error(f"æ¾å¼€é¼ æ ‡å·¦é”®å¤±è´¥: {str(e)}")
            
            logger.info("æ»‘å—æ‹–åŠ¨å®Œæˆ")
            
            # ç­‰å¾…éªŒè¯ç»“æœ
            time.sleep(5)
            verification_success = False
            
            # ç®€åŒ–éªŒè¯ç»“æœæ£€æµ‹
            # è·å–é¡µé¢æ–‡æœ¬å†…å®¹
            current_url = page.url
            logger.info(f"å½“å‰URL: {current_url}")

            if "zu.ke.com/zufang" in current_url and "captcha" not in current_url:
                verification_success = True
                logger.info(f"å·²ç»æˆåŠŸè·³è½¬åˆ°ç§Ÿæˆ¿åˆ—è¡¨é¡µé¢: {current_url}")
            
            # # åˆ¤æ–­æ˜¯å¦é€šè¿‡éªŒè¯
            # verification_success = "éªŒè¯æˆåŠŸ" in page_text or "captcha success" in page_text.lower()
            
            # # æ£€æŸ¥æ˜¯å¦è¿˜å­˜åœ¨éªŒè¯ç ç›¸å…³å†…å®¹ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™è®¤ä¸ºéªŒè¯é€šè¿‡ï¼‰
            # has_captcha = "éªŒè¯" in page_text or "captcha" in page_text.lower() or "geetest" in page_text.lower()
            
            # # å¦‚æœé¡µé¢ä¸Šä¸å†åŒ…å«éªŒè¯ç ç›¸å…³å†…å®¹ï¼Œä¹Ÿå¯èƒ½æ˜¯éªŒè¯é€šè¿‡äº†
            # if not has_captcha:
            #     verification_success = True
            #     logger.info("é¡µé¢ä¸å†åŒ…å«éªŒè¯ç ç›¸å…³å†…å®¹ï¼ŒéªŒè¯å¯èƒ½å·²é€šè¿‡")
            
            # å¦‚æœéªŒè¯æˆåŠŸï¼Œè¿”å›ç»“æœ
            if verification_success:
                logger.info(f"éªŒè¯æˆåŠŸ [å°è¯• {attempt}]")
                logger.info(f"æˆåŠŸçš„æ»‘åŠ¨è·ç¦»: {dis}px")
                screenshot = page.get_screenshot()
                return {
                    "success": True,
                    "message": "éªŒè¯æˆåŠŸ",
                    "method": "pixel_diff",
                    "distance": dis,
                    "screenshot": screenshot is not None,
                    "attempts": attempt
                }
            
            # éªŒè¯å¤±è´¥ï¼Œç»§ç»­å¾ªç¯
            logger.warning(f"éªŒè¯å¤±è´¥ [å°è¯• {attempt}]")
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´åç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•
            logger.info(f"ç­‰å¾…åè¿›è¡Œä¸‹ä¸€æ¬¡å°è¯• [{attempt}]")
            time.sleep(2)
                    
        except Exception as e:
            logger.error(f"æ‹–åŠ¨æ»‘å—å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            # å¤±è´¥åç»§ç»­å°è¯•

def handle_captcha_with_manager(driver, task_id, city_code, page_url):
    """ä½¿ç”¨éªŒè¯ç ä»£ç†ç³»ç»Ÿå¤„ç†éªŒè¯ç é¡µé¢"""
    try:
        logger.info(f"å¼€å§‹å¤„ç†éªŒè¯ç ï¼Œä»»åŠ¡ID: {task_id}, åŸå¸‚ä»£ç : {city_code}, é¡µé¢URL: {page_url}")
        
        # è®°å½•å¤„ç†éªŒè¯ç çš„å¼€å§‹æ—¶é—´
        captcha_start_time = datetime.datetime.now()
        
        # å…ˆç¡®ä¿å½“å‰çº¿ç¨‹çš„driverå·²æ³¨å†Œ
        thread_id = register_driver(driver)
        logger.info(f"å·²æ³¨å†Œçº¿ç¨‹ID: {thread_id} çš„DrissionPageå®ä¾‹")
        
        # ===å°è¯•è‡ªåŠ¨è§£å†³éªŒè¯ç ===
        logger.info("å°è¯•è‡ªåŠ¨è§£å†³éªŒè¯ç ...")
        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©é¡µé¢å®Œå…¨åŠ è½½
        time.sleep(5)
        
        result = solve_slider_captcha(driver)
        captcha_success = False
        
        if result['success']:
            logger.info("è‡ªåŠ¨è§£å†³éªŒè¯ç æˆåŠŸ!")
            captcha_success = True
        else:
            logger.warning("è‡ªåŠ¨è§£å†³éªŒè¯ç å¤±è´¥")
            
            # æç¤ºç”¨æˆ·æ‰‹åŠ¨å¤„ç†
            logger.info("=========================================================================")
            logger.info("è‡ªåŠ¨è§£å†³éªŒè¯ç å¤±è´¥ï¼è¯·æ‰‹åŠ¨å¤„ç†éªŒè¯ç ")
            logger.info("=========================================================================")
            
            # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å¤„ç†
            input("è¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼Œå®ŒæˆåæŒ‰Enteré”®ç»§ç»­...")
            captcha_success = True
            
        # è®¡ç®—å¤„ç†éªŒè¯ç æ‰€ç”¨çš„æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        if captcha_success:
            captcha_end_time = datetime.datetime.now()
            captcha_duration = (captcha_end_time - captcha_start_time).total_seconds() / 60.0
            captcha_minutes = int(captcha_duration) + 1  # å‘ä¸Šå–æ•´ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ—¶é—´
            
            logger.info(f"å¤„ç†éªŒè¯ç ç”¨æ—¶ {captcha_duration:.2f} åˆ†é’Ÿï¼Œå°†å»¶é•¿é”è¶…æ—¶æ—¶é—´ {captcha_minutes} åˆ†é’Ÿ")
            
            # å»¶é•¿é”çš„è¶…æ—¶æ—¶é—´
            if extend_crawler_lock(task_id, captcha_minutes):
                logger.info(f"æˆåŠŸå»¶é•¿çˆ¬è™«é”è¶…æ—¶æ—¶é—´ {captcha_minutes} åˆ†é’Ÿ")
            else:
                logger.warning("å»¶é•¿çˆ¬è™«é”è¶…æ—¶æ—¶é—´å¤±è´¥ï¼Œå¯èƒ½ä¼šå¯¼è‡´çˆ¬è™«è¿‡æ—©é‡Šæ”¾é”")
        
        return captcha_success
            
    except Exception as e:
        logger.error(f"å¤„ç†éªŒè¯ç æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def setup_driver():
    """è®¾ç½®å¹¶è¿”å›DrissionPageå¯¹è±¡ï¼Œæ›¿ä»£åŸSelenium WebDriver"""
    try:
        logger.info("åˆå§‹åŒ–DrissionPageæ›¿ä»£Selenium WebDriver")
        
        # åˆå§‹åŒ–DrissionPageå¯¹è±¡å‰ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»£ç†å¯ç”¨
        proxy_config = None
        proxy = ip_manager.get_random_proxy()
        if proxy:
            proxy_url = proxy["http"]
            logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_url}")
            
            # ä»ä»£ç†URLä¸­æå–ä¿¡æ¯
            if '@' in proxy_url:
                # æœ‰ç”¨æˆ·åå¯†ç çš„æƒ…å†µ: http://username:password@ip:port
                credentials, address = proxy_url.split('@')
                scheme, auth = credentials.split('://')
                username, password = auth.split(':')
                ip, port = address.split(':')
                
                # ä¸ºDrissionPageæ·»åŠ ä»£ç†è®¾ç½®
                proxy_config = {
                    'http': {
                        'server': f'{ip}:{port}',
                        'username': username,
                        'password': password
                    },
                    'https': {
                        'server': f'{ip}:{port}',
                        'username': username,
                        'password': password
                    }
                }
            else:
                # æ— ç”¨æˆ·åå¯†ç çš„æƒ…å†µ: http://ip:port
                scheme, ip_port = proxy_url.split('://')
                ip, port = ip_port.split(':')
                
                # ä¸ºDrissionPageæ·»åŠ ä»£ç†è®¾ç½®
                proxy_config = {
                    'http': {'server': f'{ip}:{port}'},
                    'https': {'server': f'{ip}:{port}'}
                }
        
        # æ ¹æ®æ˜¯å¦æœ‰ä»£ç†ä¿¡æ¯ï¼Œåˆ›å»ºDrissionPageå¯¹è±¡
        # æ£€æŸ¥ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦ä½¿ç”¨headlessæ¨¡å¼
        use_headless = os.getenv('CHROME_HEADLESS', 'false').lower() == 'true'
        logger.info(f"Chrome headlessæ¨¡å¼: {use_headless}")
        
        if proxy_config:
            # è®¾ç½®ä»£ç†ç›¸å…³é…ç½®
            co = ChromiumOptions()
            # æ ¹æ®ç¯å¢ƒå˜é‡é…ç½®headlessæ¨¡å¼
            co.headless(use_headless)
            # æ·»åŠ Dockerç¯å¢ƒæ‰€éœ€çš„é…ç½®
            co.add_argument('--no-sandbox')
            co.add_argument('--disable-dev-shm-usage')
            co.add_argument('--disable-gpu')
            co.add_argument('--remote-debugging-port=9222')
            co.set_proxy(proxy_config)
            page = ChromiumPage(co)
            mode_text = "headless" if use_headless else "ç•Œé¢"
            logger.info(f"DrissionPageå·²é…ç½®ä»£ç†å’Œ{mode_text}æ¨¡å¼")
        else:
            # é»˜è®¤é…ç½®
            co = ChromiumOptions()
            # æ ¹æ®ç¯å¢ƒå˜é‡é…ç½®headlessæ¨¡å¼
            co.headless(use_headless)
            # æ·»åŠ Dockerç¯å¢ƒæ‰€éœ€çš„é…ç½®
            co.add_argument('--no-sandbox')
            co.add_argument('--disable-dev-shm-usage')
            co.add_argument('--disable-gpu')
            co.add_argument('--remote-debugging-port=9222')
            page = ChromiumPage(co)
            mode_text = "headless" if use_headless else "ç•Œé¢"
            logger.info(f"DrissionPageå·²é…ç½®{mode_text}æ¨¡å¼")
        
        # è®¾ç½®çª—å£å¤§å°
        page.set.window.size(1920, 1080)
        
        # ä½¿ç”¨execute_scriptä¿æŒä¸selenium APIçš„å…¼å®¹æ€§
        page.run_js("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # è·å–å½“å‰å®é™…çª—å£å°ºå¯¸
        window_size = page.run_js("""
            return {
                width: window.outerWidth,
                height: window.outerHeight
            };
        """)
        window_width = window_size['width']
        window_height = window_size['height']
        
        # è®°å½•å®é™…çª—å£å¤§å°
        logger.info(f"æµè§ˆå™¨çª—å£å¤§å°: {window_width}x{window_height}")
        
        # è·å–å®é™…è§†å£å¤§å°
        viewport_size = page.run_js("""
            return {
                width: window.innerWidth,
                height: window.innerHeight
            };
        """)
        viewport_width = viewport_size['width']
        viewport_height = viewport_size['height']
        logger.info(f"æµè§ˆå™¨è§†å£å¤§å°: {viewport_width}x{viewport_height}")
        
        # ä¿å­˜å®é™…åˆ†è¾¨ç‡åˆ°å…¨å±€å˜é‡ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨
        global BROWSER_WINDOW_WIDTH, BROWSER_WINDOW_HEIGHT
        BROWSER_WINDOW_WIDTH = window_width
        BROWSER_WINDOW_HEIGHT = window_height
        
        logger.info(f"å·²è®¾ç½®æµè§ˆå™¨çª—å£å¤§å°ä¸º {window_width}x{window_height} ç”¨äºåæ ‡æ˜ å°„")
        
        return page
    except Exception as e:
        logger.error(f"è®¾ç½®DrissionPageå¯¹è±¡æ—¶å‡ºé”™: {str(e)}")
        return None

def get_total_pages(driver, base_url=None):
    """è·å–æ€»é¡µæ•°"""
    try:
        # å¦‚æœæä¾›äº†base_urlï¼Œå…ˆè®¿é—®è¯¥URL
        if base_url:
            logger.info(f"è®¿é—®URLè·å–æ€»é¡µæ•°: {base_url}")
            driver.get(base_url)
            time.sleep(random.uniform(2, 4))
            
        # æ£€æŸ¥æ˜¯å¦å‡ºç°éªŒè¯ç 
        if is_captcha_page(driver):
            logger.warning("è·å–æ€»é¡µæ•°æ—¶é‡åˆ°éªŒè¯ç é¡µé¢ï¼Œè¿”å›é»˜è®¤å€¼")
            return 1
            
        # æŸ¥æ‰¾åˆ†é¡µå…ƒç´ 
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, '.content__list--item'))
            )
            
            # å°è¯•è·å–åˆ†é¡µä¿¡æ¯
            pagination = driver.find_elements(By.CSS_SELECTOR, '.content__pg')
            if pagination:
                # è·å–æœ€åä¸€ä¸ªé¡µç æŒ‰é’®
                page_links = pagination[0].find_elements(By.CSS_SELECTOR, 'a')
                if page_links:
                    # é€šå¸¸å€’æ•°ç¬¬äºŒä¸ªæ˜¯æœ€åä¸€é¡µ
                    last_page_text = page_links[-2].text
                    if last_page_text.isdigit():
                        return int(last_page_text)
        except Exception as e:
            logger.warning(f"è·å–åˆ†é¡µä¿¡æ¯å‡ºé”™: {str(e)}")
            
        # å¦‚æœä¸Šé¢çš„æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä»div.content__pgä¸­è·å–data-totalpageå±æ€§
        try:
            pagination_div = driver.find_element(By.CSS_SELECTOR, 'div.content__pg')
            total_pages_attr = pagination_div.get_attribute('data-totalpage')
            if total_pages_attr and total_pages_attr.isdigit():
                return int(total_pages_attr)
        except:
            pass
            
        # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        logger.warning("æ— æ³•è·å–æ€»é¡µæ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼1")
        return 1
    except Exception as e:
        logger.error(f"è·å–æ€»é¡µæ•°æ—¶å‡ºç°é”™è¯¯: {str(e)}")
        return 1

def extract_house_info(house_element, city_code, task_id=None):
    """
    ä»æˆ¿æºå…ƒç´ ä¸­æå–ä¿¡æ¯
    è¿”å›æå–åˆ°çš„æˆ¿æºæ•°æ®å­—å…¸
    """
    try:
        # æå–æ ‡é¢˜ - ä½¿ç”¨DrissionPageçš„API
        title_element = house_element.ele('css:.content__list--item--title a')
        title = title_element.text.strip()
        url = title_element.attr('href')
        
        # æå–å›¾ç‰‡URL
        image_url = ""
        try:
            # å°è¯•é€šè¿‡XPathè·å–å›¾ç‰‡
            img_element = house_element.ele('css:.content__list--item--aside img')
            if img_element:
                image_url = img_element.attr('src')
            else:
                logger.warning("æå–å›¾ç‰‡URLæ—¶å‡ºé”™: æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡å…ƒç´ ")
        except Exception as img_error:
            logger.warning(f"æå–å›¾ç‰‡URLæ—¶å‡ºé”™: {str(img_error)}")
        
        # æå–ä»·æ ¼
        price_element = house_element.ele('css:.content__list--item-price')
        price_text = price_element.text.strip()
        # print(price_text)
        price = int(re.search(r'\d+', price_text).group()) if re.search(r'\d+', price_text) else 0
        
        # æå–æˆ·å‹ã€é¢ç§¯ç­‰ä¿¡æ¯
        desc_elements = house_element.eles('css:.content__list--item--des')
        # print(desc_elements.text)
        # åˆå§‹åŒ–å˜é‡
        layout = ""
        area = 0
        floor = ""
        direction = ""
        district = ""
        community = ""
        subway = ""

        desc_text = desc_elements[0].text
        # ç”¨ / æˆ– \n åˆ†å‰²æˆè‹¥å¹²éƒ¨åˆ†
        desc_parts = [part.strip() for part in re.split(r'[\/\n]', desc_text) if part.strip()]
        for part in desc_parts:
            # print(f"ğŸ§© part: {part}")

            # åŒºåŸŸå’Œå°åŒºï¼ˆå¦‚ï¼šæµ¦ä¸œ-å¼ æ±Ÿ-ç«ç‘°æ¹¾(åˆ«å¢…)ï¼‰
            if "-" in part and not district:
                location_parts = part.split("-")
                if len(location_parts) >= 2:
                    district = location_parts[0]
                    community = location_parts[-1]

            # é¢ç§¯
            elif "ã¡" in part and not area:
                area_match = re.search(r'(\d+(\.\d+)?)', part)
                if area_match:
                    area = float(area_match.group(1))

            # æˆ·å‹
            elif "å®¤" in part and "å…" in part and not layout:
                layout = part

            # æ¥¼å±‚
            elif "æ¥¼å±‚" in part and not floor:
                floor = part

            # æœå‘ï¼ˆåªåˆ¤æ–­æ–¹å‘çš„æ±‰å­—ï¼‰
            elif any(d in part for d in ["ä¸œ", "å—", "è¥¿", "åŒ—"]) and not direction:
                direction = part

            # åœ°é“
            elif "å·çº¿" in part and not subway:
                subway = part
        
        publish_date = datetime.datetime.now().strftime("%Y-%m-%d")
        publish_elements = house_element.eles('css:.content__list--item--time')

        if publish_elements:
            publish_text = publish_elements[0].text.strip()

            # 1. æ ¼å¼ï¼š3å¤©å‰ç»´æŠ¤
            match_before = re.search(r'(\d+)å¤©å‰', publish_text)
            if match_before:
                days = int(match_before.group(1))
                publish_date = (datetime.datetime.now() - datetime.timedelta(days=days)).strftime('%Y-%m-%d')

            # 3. æ ¼å¼ï¼šä»Šå¤©ç»´æŠ¤
            elif "ä»Šå¤©ç»´æŠ¤" in publish_text:
                publish_date = datetime.datetime.now().strftime('%Y-%m-%d')

            # 4. æ ¼å¼ï¼š05.12å‘å¸ƒ
            else:
                date_match = re.search(r'(\d{2})\.(\d{2})', publish_text)
                if date_match:
                    year = datetime.datetime.now().year
                    month, day = date_match.groups()
                    publish_date = f"{year}-{month}-{day}"
        else:
            publish_date = None
        
        # æå–ç‰¹è‰²æ ‡ç­¾
        # features = []
        tag_elements = house_element.eles('css:.content__list--item--bottom i')
        features = [el.text.strip() for el in tag_elements]
        print({
            'url': url,
            'title': title,
            'price': price,
            'layout': layout,
            'area': area,
            'floor': floor,
            'direction': direction,
            'subway': subway,
            'district': district,
            'community': community,
            'city_code': city_code,
            'publish_date': publish_date,
            'features': features,
            'task_id': task_id,
            'image_url': image_url  # æ·»åŠ å›¾ç‰‡URL
        })
        # æ„å»ºå¹¶è¿”å›æˆ¿æºä¿¡æ¯å­—å…¸
        return {
            'url': url,
            'title': title,
            'price': price,
            'layout': layout,
            'area': area,
            'floor': floor,
            'direction': direction,
            'subway': subway,
            'district': district,
            'community': community,
            'city_code': city_code,
            'publish_date': publish_date,
            'features': features,
            'task_id': task_id,
            'image_url': image_url  # æ·»åŠ å›¾ç‰‡URL
        }
    
    except Exception as e:
        logger.error(f"æå–æˆ¿æºä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None

def crawl_city_with_selenium(city_name, city_code, max_pages=5, task_id=None):
    """ä½¿ç”¨DrissionPageçˆ¬å–æŒ‡å®šåŸå¸‚çš„æˆ¿æºä¿¡æ¯ï¼Œå¯ä»¥ä»å·²æœ‰ä»»åŠ¡ç»§ç»­
    
    Args:
        city_name: åŸå¸‚åç§°
        city_code: åŸå¸‚ä»£ç 
        max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
        task_id: å·²æœ‰ä»»åŠ¡IDï¼Œå¦‚æœæä¾›åˆ™ç»§ç»­è¯¥ä»»åŠ¡
    """
    # é‡è¯•é…ç½®
    MAX_RETRIES = 3
    
    try:
        logger.info(f"å¼€å§‹ä½¿ç”¨DrissionPageçˆ¬å– {city_name}({city_code}) çš„æˆ¿æºä¿¡æ¯ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")
        
        if task_id:
            # å¦‚æœæä¾›äº†ä»»åŠ¡IDï¼Œåˆ™éªŒè¯å®ƒæ˜¯å¦å­˜åœ¨
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM crawl_task WHERE id = %s", (task_id,))
            existing_task = cursor.fetchone()
            cursor.close()
            conn.close()
            
            if existing_task:
                logger.info(f"ç»§ç»­å·²æœ‰ä»»åŠ¡ ID: {task_id}")
            else:
                logger.warning(f"æä¾›çš„ä»»åŠ¡ID {task_id} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°ä»»åŠ¡")
                task_id = None
                
        if not task_id:
            # åˆ›å»ºæ–°ä»»åŠ¡
            task_id = start_crawl_task(city_name, city_code)
            logger.info(f"åˆ›å»ºæ–°çš„çˆ¬å–ä»»åŠ¡ ID: {task_id}")
        # ç”±çˆ¬è™«è¿›ç¨‹è¡¥å…¨è®¡åˆ’å­—æ®µ
        update_crawl_task(
            task_id=task_id,
            status="In Progress",
            total_pages=max_pages,
            expected_items=max_pages * 30
        )
        
        # åˆå§‹åŒ–æµè§ˆå™¨
        driver = setup_driver()
        
        try:
            # æ³¨å†Œå½“å‰çº¿ç¨‹çš„WebDriverå¯¹è±¡
            register_driver(driver)
            
            # è®¾ç½®åŸºç¡€URL
            base_url = f"https://{city_code}.zu.ke.com/zufang/"
            
            # ç›´æ¥ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„é¡µæ•°ï¼Œä¸å†å°è¯•è·å–ç½‘ç«™çš„æ€»é¡µæ•°
            pages_to_crawl = max_pages
            logger.info(f"è®¡åˆ’çˆ¬å–: {pages_to_crawl} é¡µ")
            
            # è·å–å·²æˆåŠŸçˆ¬å–çš„é¡µé¢ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
            crawled_pages = get_crawled_pages(task_id)
            logger.info(f"æ£€æµ‹åˆ°å·²æˆåŠŸçˆ¬å–çš„é¡µé¢: {crawled_pages}" if crawled_pages else "æœªæ£€æµ‹åˆ°å·²çˆ¬å–é¡µé¢ï¼Œå°†ä»ç¬¬1é¡µå¼€å§‹çˆ¬å–")
            
            for page in range(1, pages_to_crawl + 1):
                # å¦‚æœé¡µé¢å·²ç»æˆåŠŸçˆ¬å–è¿‡ï¼Œåˆ™è·³è¿‡
                if page in crawled_pages:
                    logger.info(f"ç¬¬ {page} é¡µå·²æˆåŠŸçˆ¬å–ï¼Œè·³è¿‡")
                    continue
                
                page_url = f"{base_url}pg{page}/"
                logger.info(f"å¼€å§‹çˆ¬å–ç¬¬ {page} é¡µ: {page_url}")
                
                # é‡è¯•æœºåˆ¶
                retry_count = 0
                success = False
                last_error = None
                
                while retry_count < MAX_RETRIES and not success:
                    if retry_count > 0:
                        logger.warning(f"ç¬¬ {page} é¡µçˆ¬å–å¤±è´¥ï¼Œè¿›è¡Œç¬¬ {retry_count} æ¬¡é‡è¯•")
                        # æ¯æ¬¡é‡è¯•å¢åŠ ç­‰å¾…æ—¶é—´
                        time.sleep(5 * retry_count)
                    
                    try:
                        # è®¿é—®é¡µé¢
                        driver.get(page_url)
                        time.sleep(random.uniform(2, 5))  # éšæœºç­‰å¾…
                        
                        # æ£€æŸ¥æ˜¯å¦å‡ºç°éªŒè¯ç 
                        if is_captcha_page(driver):
                            logger.warning(f"ç¬¬ {page} é¡µå‡ºç°éªŒè¯ç ï¼Œå¼€å§‹å¤„ç†")
                            
                            # ä½¿ç”¨éªŒè¯ç ä»£ç†ç³»ç»Ÿå¤„ç†éªŒè¯ç 
                            if handle_captcha_with_manager(driver, task_id, city_code, page_url):
                                logger.info("éªŒè¯ç å¤„ç†æˆåŠŸï¼Œç»§ç»­çˆ¬å–")
                                # é‡æ–°åŠ è½½é¡µé¢
                                driver.get(page_url)
                                time.sleep(random.uniform(2, 5))
                            else:
                                raise Exception("éªŒè¯ç å¤„ç†å¤±è´¥")
                        
                        # ç­‰å¾…æˆ¿æºåˆ—è¡¨åŠ è½½ (é€šè¿‡sleepæ›¿ä»£WebDriverWaitï¼Œå› ä¸ºDrissionPageæ²¡æœ‰WebDriverWait)
                        time.sleep(3)
                        
                        # å¤„ç†è¯¥é¡µçš„æ‰€æœ‰æˆ¿æº
                        success_count, total_count = process_house_items(driver, task_id)
                        logger.info(f"ç¬¬ {page} é¡µæˆåŠŸä¿å­˜ {success_count}/{total_count} ä¸ªæˆ¿æº")
                        
                        # æ ‡è®°ä¸ºæˆåŠŸ
                        success = success_count > 0
                        
                        # è®°å½•è¯¥é¡µçˆ¬å–çš„æˆåŠŸæˆ¿æºæ•° - ä¿®å¤è¿æ¥ç®¡ç†é—®é¢˜
                        try:
                            # è·å–ç»Ÿè®¡ä¿¡æ¯ - è¿™ä¸ªå‡½æ•°å†…éƒ¨ä¼šç®¡ç†è‡ªå·±çš„è¿æ¥
                            stats = get_crawl_statistics(task_id)
                            logger.info(f"ç›®å‰å·²æˆåŠŸçˆ¬å– {stats['task_items']} æ¡æˆ¿æºä¿¡æ¯ï¼ˆæ€»æ•°æ®åº“è®°å½•ï¼š{stats['total_items']}æ¡ï¼‰")
                            
                            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ - è¿™ä¸ªå‡½æ•°ä½¿ç”¨è£…é¥°å™¨ç®¡ç†è¿æ¥
                            update_crawl_task(
                                task_id=task_id, 
                                status="In Progress", 
                                success_items=stats['task_items'],
                                success_pages=stats['success_pages']
                            )
                        except Exception as e:
                            logger.error(f"æ›´æ–°çˆ¬å–è¿›åº¦æ—¶å‡ºé”™: {str(e)}")
                        # ç§»é™¤å¤šä½™çš„å¼‚å¸¸å¤„ç†å’Œè¿æ¥ç®¡ç†ä»£ç ï¼Œé¿å…å°è¯•å½’è¿˜ä¸å­˜åœ¨çš„è¿æ¥
                        
                    except Exception as e:
                        last_error = str(e)
                        logger.error(f"çˆ¬å–ç¬¬ {page} é¡µæ—¶å‡ºé”™ (å°è¯• {retry_count+1}/{MAX_RETRIES}): {last_error}")
                    
                    retry_count += 1
                
                # è®°å½•é¡µé¢çˆ¬å–çŠ¶æ€
                record_page_crawl(
                    task_id, 
                    page, 
                    page_url, 
                    success=success, 
                    retry_count=retry_count-1, 
                    error_message=None if success else last_error
                )
                
                if not success:
                    logger.error(f"ç¬¬ {page} é¡µçˆ¬å–å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {MAX_RETRIES}")
                
                # éšæœºç­‰å¾…ï¼Œé¿å…è¢«å°
                time.sleep(random.uniform(5, 10))
            
            logger.info(f"å®Œæˆ {city_name} çš„çˆ¬å–ä»»åŠ¡ï¼Œå…±è®¡åˆ’çˆ¬å– {pages_to_crawl} é¡µ")
            
            # æŸ¥è¯¢å·²æˆåŠŸçˆ¬å–çš„é¡µé¢æ•°
            successful_pages = get_crawled_pages(task_id)
            logger.info(f"å…±æˆåŠŸçˆ¬å– {len(successful_pages)}/{pages_to_crawl} é¡µ")
            
            # å¦‚æœå…¨éƒ¨é¡µé¢éƒ½çˆ¬å–æˆåŠŸï¼Œæ ‡è®°ä»»åŠ¡ä¸ºå®Œæˆ
            if len(successful_pages) == pages_to_crawl:
                # è®¡ç®—æ€»æˆåŠŸçˆ¬å–çš„æˆ¿æºæ•°
                try:
                    stats = get_crawl_statistics(task_id)
                    logger.info(f"ä»»åŠ¡ {task_id} å…±æˆåŠŸçˆ¬å– {stats['task_items']} æ¡æˆ¿æºä¿¡æ¯")
                    
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ŒåŒ…æ‹¬æˆåŠŸçˆ¬å–çš„æˆ¿æºæ•°é‡
                    update_crawl_task(
                        task_id=task_id, 
                        status="Completed", 
                        success_items=stats['task_items'],
                        success_pages=len(successful_pages),
                        total_pages=pages_to_crawl
                    )
                    
                    # çˆ¬è™«ä»»åŠ¡å®Œæˆåè‡ªåŠ¨æ‰§è¡Œæ•°æ®åˆ†æ
                    logger.info(f"çˆ¬è™«ä»»åŠ¡å®Œæˆï¼Œå‡†å¤‡æ‰§è¡Œæ•°æ®åˆ†æ...")
                    # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œæ•°æ®åˆ†æï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
                    analysis_thread = threading.Thread(
                        target=run_data_analysis,
                        args=(task_id, city_name, city_code),
                        daemon=True
                    )
                    analysis_thread.start()
                    logger.info(f"æ•°æ®åˆ†æä»»åŠ¡å·²åœ¨åå°å¯åŠ¨")
                except Exception as e:
                    logger.error(f"è®¡ç®—çˆ¬å–æˆ¿æºæ•°æ—¶å‡ºé”™: {str(e)}")
                    update_crawl_task(task_id=task_id, status="Completed")
            else:
                # ä»»åŠ¡æœªå®Œå…¨å®Œæˆï¼Œä½†æš‚æ—¶ç»“æŸ
                try:
                    stats = get_crawl_statistics(task_id)
                    logger.info(f"ä»»åŠ¡ {task_id} éƒ¨åˆ†å®Œæˆï¼Œå…±çˆ¬å– {stats['task_items']} æ¡æˆ¿æºä¿¡æ¯")
                    
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ŒåŒ…æ‹¬æˆåŠŸçˆ¬å–çš„æˆ¿æºæ•°é‡
                    update_crawl_task(
                        task_id=task_id, 
                        status="Incomplete", 
                        success_items=stats['task_items'],
                        success_pages=len(successful_pages),
                        total_pages=pages_to_crawl,
                        error_message=f"éƒ¨åˆ†é¡µé¢çˆ¬å–å¤±è´¥ï¼Œå·²æˆåŠŸçˆ¬å– {len(successful_pages)}/{pages_to_crawl} é¡µ"
                    )
                except Exception as e:
                    logger.error(f"è®¡ç®—çˆ¬å–æˆ¿æºæ•°æ—¶å‡ºé”™: {str(e)}")
                    update_crawl_task(
                        task_id=task_id, 
                        status="Incomplete", 
                        error_message=f"éƒ¨åˆ†é¡µé¢çˆ¬å–å¤±è´¥ï¼Œå·²æˆåŠŸçˆ¬å– {len(successful_pages)}/{pages_to_crawl} é¡µ"
                    )
            
        finally:
            # å…³é—­æµè§ˆå™¨
            unregister_driver()
            # ä½¿ç”¨DrissionPageçš„å…³é—­æ–¹æ³•
            driver.quit()
            
    except Exception as e:
        logger.error(f"DrissionPageçˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        if task_id:
            try:
                conn = connection_pool.getconn()
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT COUNT(*) FROM house_info WHERE task_id = %s",
                    (task_id,)
                )
                success_items_count = cursor.fetchone()[0]
                logger.info(f"ä»»åŠ¡ {task_id} å¤±è´¥ï¼Œä½†å·²çˆ¬å– {success_items_count} æ¡æˆ¿æºä¿¡æ¯")
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ŒåŒ…æ‹¬æˆåŠŸçˆ¬å–çš„æˆ¿æºæ•°é‡
                update_crawl_task(task_id, "Failed", 
                                success_items=success_items_count,
                                error_message=str(e))
                
                # ç¡®ä¿è¿æ¥æ­£ç¡®å…³é—­
                connection_pool.putconn(conn)
            except Exception as count_error:
                logger.error(f"è®¡ç®—çˆ¬å–æˆ¿æºæ•°æ—¶å‡ºé”™: {str(count_error)}")
                update_crawl_task(task_id, "Failed", error_message=str(e))
        
    return task_id

def save_browser_cookies(driver, session_id):
    """ä¿å­˜æµè§ˆå™¨cookiesåˆ°éªŒè¯ä¼šè¯"""
    try:
        cookies = driver.get_cookies()
        verification_manager.save_verification_cookies(session_id, cookies)
        logger.info(f"ä¿å­˜æµè§ˆå™¨cookiesåˆ°éªŒè¯ä¼šè¯ {session_id} æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜æµè§ˆå™¨cookieså¤±è´¥: {str(e)}")
        return False

def export_to_csv(houses, filename="rental_data.csv"):
    """å°†æˆ¿æºæ•°æ®å¯¼å‡ºä¸ºCSVæ–‡ä»¶"""
    if not houses:
        logger.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        return False
    
    try:
        df = pd.DataFrame(houses)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        logger.info(f"æ•°æ®å·²å¯¼å‡ºåˆ° {filename}")
        return True
    except Exception as e:
        logger.error(f"å¯¼å‡ºæ•°æ®å¤±è´¥: {str(e)}")
        return False

def get_city_code(city_name):
    """
    æ ¹æ®åŸå¸‚åç§°è·å–åŸå¸‚ä»£ç 
    
    Args:
        city_name (str): åŸå¸‚åç§°ï¼Œå¦‚"åŒ—äº¬"ã€"ä¸Šæµ·"ç­‰
        
    Returns:
        str: åŸå¸‚ä»£ç ï¼Œå¦‚"bj"ã€"sh"ç­‰ï¼›å¦‚æœåŸå¸‚ä¸æ”¯æŒï¼Œè¿”å›None
    """
    cities = get_supported_cities()
    return cities.get(city_name)

def get_supported_cities():
    """è·å–æ”¯æŒçš„åŸå¸‚åˆ—è¡¨"""
    return {
        "åŒ—äº¬": "bj",
        "ä¸Šæµ·": "sh",
        "å¹¿å·": "gz",
        "æ·±åœ³": "sz",
        "æ­å·": "hz",
        "å—äº¬": "nj",
        "æˆéƒ½": "cd",
        "æ­¦æ±‰": "wh",
        "å¤©æ´¥": "tj",
        "è¥¿å®‰": "xa",
        "é‡åº†": "cq",
        "è‹å·": "su",
        "éƒ‘å·": "zz",
        "é•¿æ²™": "cs",
        "åˆè‚¥": "hf",
        "å®æ³¢": "nb",
        "é’å²›": "qd",
        "å¤§è¿": "dl",
        "å¦é—¨": "xm",
        "ç¦å·": "fz",
        "æµå—": "jn",
        "å—æ˜Œ": "nc",
        "æ˜†æ˜": "km",
        "æ²ˆé˜³": "sy",
        "é•¿æ˜¥": "cc",
        "å“ˆå°”æ»¨": "hrb",
        "çŸ³å®¶åº„": "sjz",
        "å¤ªåŸ": "ty",
        "å—å®": "nn",
        "æ— é”¡": "wx",
        "æ¹–å·": "huzhou",
        "å¸¸å·": "cz",
        "å˜‰å…´": "jx",
        "æµ·å£": "hk",
        "è´µé˜³": "gy",
        "ä¸‰äºš": "sanya",
        "å…°å·": "lz",
        "å»ŠåŠ": "lf",
        "ä¿å®š": "bd",
        "ä½›å±±": "fs",
        "ä¸œè": "dg",
        "ä¸­å±±": "zs",
        "ç æµ·": "zh",
        "æ¹›æ±Ÿ": "zhanjiang"
    }

def record_page_crawl(task_id, page_number, page_url, success=True, retry_count=0, error_message=None):
    """è®°å½•é¡µé¢çˆ¬å–çŠ¶æ€ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
    
    Args:
        task_id: çˆ¬è™«ä»»åŠ¡ID
        page_number: é¡µç 
        page_url: é¡µé¢URL
        success: æ˜¯å¦æˆåŠŸçˆ¬å–
        retry_count: é‡è¯•æ¬¡æ•°
        error_message: é”™è¯¯ä¿¡æ¯
    """
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•è®°å½•é¡µé¢çˆ¬å–çŠ¶æ€")
        return False
    
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®°å½•
        cursor.execute(
            "SELECT id FROM crawled_pages WHERE task_id = %s AND page_number = %s",
            (task_id, page_number)
        )
        exists = cursor.fetchone()
        
        current_time = datetime.datetime.now()
        
        if exists:
            # æ›´æ–°å·²æœ‰è®°å½•
            cursor.execute(
                """
                UPDATE crawled_pages 
                SET success = %s, retry_count = %s, error_message = %s, crawl_time = %s 
                WHERE task_id = %s AND page_number = %s
                """,
                (success, retry_count, error_message, current_time, task_id, page_number)
            )
        else:
            # æ’å…¥æ–°è®°å½•
            cursor.execute(
                """
                INSERT INTO crawled_pages 
                (task_id, page_number, page_url, crawl_time, success, retry_count, error_message)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                """,
                (task_id, page_number, page_url, current_time, success, retry_count, error_message)
            )
        
        conn.commit()
        return True
    except Exception as e:
        logger.error(f"è®°å½•é¡µé¢çˆ¬å–çŠ¶æ€å¤±è´¥: {str(e)}")
        return False
    finally:
        if conn:
            connection_pool.putconn(conn)

def is_page_crawled_successfully(task_id, page_number):
    """æ£€æŸ¥é¡µé¢æ˜¯å¦å·²æˆåŠŸçˆ¬å–ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
    
    Args:
        task_id: çˆ¬è™«ä»»åŠ¡ID
        page_number: é¡µç 
    
    Returns:
        bool: æ˜¯å¦å·²æˆåŠŸçˆ¬å–
    """
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•æ£€æŸ¥é¡µé¢çˆ¬å–çŠ¶æ€")
        return False
    
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        cursor.execute(
            "SELECT success FROM crawled_pages WHERE task_id = %s AND page_number = %s",
            (task_id, page_number)
        )
        result = cursor.fetchone()
        
        return result is not None and result[0] == True
    except Exception as e:
        logger.error(f"æ£€æŸ¥é¡µé¢çˆ¬å–çŠ¶æ€å¤±è´¥: {str(e)}")
        return False
    finally:
        if conn:
            connection_pool.putconn(conn)

def get_crawled_pages(task_id):
    """è·å–ä»»åŠ¡å·²çˆ¬å–çš„é¡µé¢åˆ—è¡¨
    
    Args:
        task_id: çˆ¬è™«ä»»åŠ¡ID
    
    Returns:
        list: å·²æˆåŠŸçˆ¬å–çš„é¡µç åˆ—è¡¨
    """
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•è·å–å·²çˆ¬å–é¡µé¢")
        return []
    
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        try:
            # å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'crawled_pages'
                )
            """)
            table_exists = cursor.fetchone()[0]
            
            if not table_exists:
                logger.warning("crawled_pagesè¡¨ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º...")
                # åˆ›å»ºè¡¨
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS crawled_pages (
                    id SERIAL PRIMARY KEY,
                    task_id INTEGER REFERENCES crawl_task(id),
                    page_number INTEGER NOT NULL,
                    page_url TEXT NOT NULL,
                    crawl_time TIMESTAMP NOT NULL,
                    success BOOLEAN NOT NULL,
                    retry_count INTEGER DEFAULT 0,
                    error_message TEXT,
                    UNIQUE (task_id, page_number)
                )
                ''')
                conn.commit()
                logger.info("crawled_pagesè¡¨åˆ›å»ºæˆåŠŸ")
                return []  # æ–°è¡¨ä¸­æ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºåˆ—è¡¨
        
            cursor.execute(
                "SELECT page_number FROM crawled_pages WHERE task_id = %s AND success = true",
                (task_id,)
            )
            results = cursor.fetchall()
            
            return [row[0] for row in results]
        except Exception as e:
            logger.error(f"è·å–å·²çˆ¬å–é¡µé¢å¤±è´¥: {str(e)}")
            # å°è¯•è¿è¡Œæ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
            try:
                # å…ˆå…³é—­å½“å‰è¿æ¥
                cursor.close()
                connection_pool.putconn(conn)
                
                # å†æ¬¡å°è¯•è·å–
                conn = connection_pool.getconn()
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT page_number FROM crawled_pages WHERE task_id = %s AND success = true",
                    (task_id,)
                )
                results = cursor.fetchall()
                
                return [row[0] for row in results]
            except Exception as init_error:
                logger.error(f"é‡è¯•åˆ›å»ºè¡¨å¹¶è·å–æ•°æ®å¤±è´¥: {str(init_error)}")
                return []
    finally:
        if 'conn' in locals() and conn:
            connection_pool.putconn(conn)

def get_browser_dimensions():
    """è·å–æµè§ˆå™¨å®é™…å°ºå¯¸"""
    return {
        'width': BROWSER_WINDOW_WIDTH,
        'height': BROWSER_WINDOW_HEIGHT
    }

def acquire_crawler_lock(task_id, max_pages=5, lock_timeout_minutes=None):
    """
    å°è¯•è·å–çˆ¬è™«é”ï¼Œå¦‚æœæˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    
    Args:
        task_id: çˆ¬è™«ä»»åŠ¡ID
        max_pages: è®¡åˆ’çˆ¬å–çš„æœ€å¤§é¡µæ•°ï¼Œç”¨äºè®¡ç®—è¶…æ—¶æ—¶é—´
        lock_timeout_minutes: é”è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ï¼Œå¦‚æœä¸æä¾›åˆ™æ ¹æ®é¡µæ•°åŠ¨æ€è®¡ç®—
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸè·å–é”
    """
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•è·å–çˆ¬è™«é”")
        return False
    
    # å¦‚æœæ²¡æœ‰æä¾›è¶…æ—¶æ—¶é—´ï¼Œåˆ™æ ¹æ®é¡µæ•°åŠ¨æ€è®¡ç®—
    if lock_timeout_minutes is None:
        # åŸºæœ¬ç®—æ³•ï¼š
        # 1. æ¯é¡µåŸºæœ¬çˆ¬å–æ—¶é—´ï¼š2.5åˆ†é’Ÿï¼ˆé¡µé¢åŠ è½½ã€æå–ä¿¡æ¯ç­‰ï¼‰
        # 2. åˆå§‹éªŒè¯ç æ—¶é—´é¢„ç•™ï¼šæ¯4é¡µé¢„ç•™15åˆ†é’Ÿï¼ˆéªŒè¯ç é€šå¸¸ä¸ä¼šæ¯é¡µéƒ½å‡ºç°ï¼‰
        # 3. é¢å¤–å¢åŠ 50%çš„ç¼“å†²æ—¶é—´ç”¨äºå¤„ç†ç½‘ç»œå»¶è¿Ÿç­‰æ„å¤–æƒ…å†µ
        # 4. å†åŠ ä¸Š15åˆ†é’Ÿçš„åŸºç¡€ç¼“å†²æ—¶é—´ç”¨äºåˆå§‹åŒ–å’Œå…¶ä»–æ“ä½œ
        # æ³¨æ„ï¼šåç»­æ¯æ¬¡å®é™…å¤„ç†éªŒè¯ç æ—¶ä¼šåŠ¨æ€å»¶é•¿é”æ—¶é—´
        base_time_per_page = 2.5  # åŸºæœ¬çˆ¬å–æ—¶é—´ï¼ˆåˆ†é’Ÿ/é¡µï¼‰
        captcha_time_allocation = max(15, (max_pages // 4) * 15)  # åˆå§‹é¢„ç•™çš„éªŒè¯ç å¤„ç†æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        buffer_ratio = 1.5  # ç¼“å†²æ¯”ä¾‹
        base_buffer = 15  # åŸºç¡€ç¼“å†²æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        
        # è®¡ç®—æ€»çš„é”è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        lock_timeout_minutes = int(base_time_per_page * max_pages * buffer_ratio) + captcha_time_allocation + base_buffer
        logger.info(f"æ ¹æ®çˆ¬å–é¡µæ•° {max_pages} åŠ¨æ€è®¡ç®—é”è¶…æ—¶æ—¶é—´ä¸º {lock_timeout_minutes} åˆ†é’Ÿï¼ŒåŒ…å«éªŒè¯ç é¢„ç•™æ—¶é—´ {captcha_time_allocation} åˆ†é’Ÿ")
        
        # å¦‚æœé¡µæ•°è¿‡å¤šï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„æ—¶é—´ï¼ˆæœ€å¤š24å°æ—¶ï¼‰
        if lock_timeout_minutes > 24 * 60:
            lock_timeout_minutes = 24 * 60
            logger.info(f"è¶…æ—¶æ—¶é—´è¶…è¿‡24å°æ—¶ï¼Œå·²é™åˆ¶ä¸º {lock_timeout_minutes} åˆ†é’Ÿ")
            
        # å¦‚æœé¡µæ•°å¾ˆå°‘ï¼Œç¡®ä¿æœ‰æœ€ä½çš„è¶…æ—¶æ—¶é—´
        if lock_timeout_minutes < 30:
            lock_timeout_minutes = 30
            logger.info(f"è¶…æ—¶æ—¶é—´è¿‡çŸ­ï¼Œå·²è®¾ç½®ä¸ºæœ€ä½å€¼ {lock_timeout_minutes} åˆ†é’Ÿ")
    
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        # å¼€å§‹äº‹åŠ¡
        conn.autocommit = False
        
        # è·å–é”çš„å½“å‰çŠ¶æ€ï¼ˆä½¿ç”¨FOR UPDATEé”å®šè¡Œï¼‰
        cursor.execute(
            "SELECT is_locked, locked_by, locked_at, expires_at FROM crawler_lock WHERE lock_name = %s FOR UPDATE",
            ('main_crawler_lock',)
        )
        lock_info = cursor.fetchone()
        
        if not lock_info:
            logger.error("çˆ¬è™«é”è®°å½•ä¸å­˜åœ¨")
            conn.rollback()
            return False
        
        is_locked, locked_by, locked_at, expires_at = lock_info
        current_time = datetime.datetime.now()
        
        # æ£€æŸ¥é”æ˜¯å¦å·²è¢«è·å–ï¼Œå¹¶ä¸”æ²¡æœ‰è¿‡æœŸ
        if is_locked and expires_at and current_time < expires_at:
            # è®¡ç®—å‰©ä½™æ—¶é—´
            remaining_minutes = (expires_at - current_time).total_seconds() / 60
            logger.info(f"çˆ¬è™«é”å·²è¢«ä»»åŠ¡ {locked_by} æŒæœ‰ï¼Œå‰©ä½™ {remaining_minutes:.1f} åˆ†é’Ÿï¼Œè·å–å¤±è´¥")
            # æäº¤äº‹åŠ¡
            conn.commit()
            return False
        
        # è®¡ç®—è¿‡æœŸæ—¶é—´
        expiration_time = current_time + datetime.timedelta(minutes=lock_timeout_minutes)
        
        # è·å–é”
        cursor.execute(
            """
            UPDATE crawler_lock
            SET is_locked = %s, locked_by = %s, locked_at = %s, expires_at = %s
            WHERE lock_name = %s
            """,
            (True, task_id, current_time, expiration_time, 'main_crawler_lock')
        )
        
        # åŒæ—¶æ›´æ–°ä»»åŠ¡çš„é¢„è®¡ç»“æŸæ—¶é—´
        cursor.execute(
            """
            UPDATE crawl_task
            SET expected_end_time = %s
            WHERE id = %s
            """,
            (expiration_time, task_id)
        )
        
        # æäº¤äº‹åŠ¡
        conn.commit()
        logger.info(f"ä»»åŠ¡ {task_id} æˆåŠŸè·å–çˆ¬è™«é”ï¼Œè¶…æ—¶æ—¶é—´ä¸º {lock_timeout_minutes} åˆ†é’Ÿï¼Œè¿‡æœŸæ—¶é—´ {expiration_time}")
        return True
    except Exception as e:
        logger.error(f"è·å–çˆ¬è™«é”å¤±è´¥: {str(e)}")
        if 'conn' in locals() and conn:
            conn.rollback()
        return False
    finally:
        if 'conn' in locals() and conn:
            conn.autocommit = True
            connection_pool.putconn(conn)

def release_crawler_lock(task_id):
    """
    é‡Šæ”¾çˆ¬è™«é”ï¼Œå¦‚æœé”ç”±å½“å‰ä»»åŠ¡æŒæœ‰
    
    Args:
        task_id: çˆ¬è™«ä»»åŠ¡ID
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸé‡Šæ”¾é”
    """
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•é‡Šæ”¾çˆ¬è™«é”")
        return False
    
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        # å¼€å§‹äº‹åŠ¡
        conn.autocommit = False
        
        # è·å–é”çš„å½“å‰çŠ¶æ€ï¼ˆä½¿ç”¨FOR UPDATEé”å®šè¡Œï¼‰
        cursor.execute(
            "SELECT is_locked, locked_by FROM crawler_lock WHERE lock_name = %s FOR UPDATE",
            ('main_crawler_lock',)
        )
        lock_info = cursor.fetchone()
        
        if not lock_info:
            logger.error("çˆ¬è™«é”è®°å½•ä¸å­˜åœ¨")
            conn.rollback()
            return False
        
        is_locked, locked_by = lock_info
        
        # æ£€æŸ¥é”æ˜¯å¦ç”±å½“å‰ä»»åŠ¡æŒæœ‰
        if is_locked and locked_by == task_id:
            # é‡Šæ”¾é”
            cursor.execute(
                """
                UPDATE crawler_lock
                SET is_locked = %s, locked_by = NULL, locked_at = NULL, expires_at = NULL
                WHERE lock_name = %s
                """,
                (False, 'main_crawler_lock')
            )
            
            # æäº¤äº‹åŠ¡
            conn.commit()
            logger.info(f"ä»»åŠ¡ {task_id} æˆåŠŸé‡Šæ”¾çˆ¬è™«é”")
            
            # å°è¯•å¯åŠ¨é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªä»»åŠ¡
            start_next_queued_task()
            
            return True
        else:
            logger.warning(f"ä»»åŠ¡ {task_id} å°è¯•é‡Šæ”¾ä¸å±äºå®ƒçš„é”")
            conn.rollback()
            return False
    except Exception as e:
        logger.error(f"é‡Šæ”¾çˆ¬è™«é”å¤±è´¥: {str(e)}")
        if 'conn' in locals() and conn:
            conn.rollback()
        return False
    finally:
        if 'conn' in locals() and conn:
            conn.autocommit = True
            connection_pool.putconn(conn)

def extend_crawler_lock(task_id, additional_minutes):
    """
    å»¶é•¿çˆ¬è™«é”çš„è¶…æ—¶æ—¶é—´
    
    Args:
        task_id: çˆ¬è™«ä»»åŠ¡ID
        additional_minutes: è¦é¢å¤–å¢åŠ çš„åˆ†é’Ÿæ•°
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸå»¶é•¿é”çš„è¶…æ—¶æ—¶é—´
    """
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•å»¶é•¿çˆ¬è™«é”è¶…æ—¶æ—¶é—´")
        return False
    
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        # å¼€å§‹äº‹åŠ¡
        conn.autocommit = False
        
        # è·å–é”çš„å½“å‰çŠ¶æ€ï¼ˆä½¿ç”¨FOR UPDATEé”å®šè¡Œï¼‰
        cursor.execute(
            "SELECT is_locked, locked_by, expires_at FROM crawler_lock WHERE lock_name = %s FOR UPDATE",
            ('main_crawler_lock',)
        )
        lock_info = cursor.fetchone()
        
        if not lock_info:
            logger.error("çˆ¬è™«é”è®°å½•ä¸å­˜åœ¨")
            conn.rollback()
            return False
        
        is_locked, locked_by, current_expires_at = lock_info
        
        # æ£€æŸ¥é”æ˜¯å¦ç”±å½“å‰ä»»åŠ¡æŒæœ‰
        if is_locked and locked_by == task_id:
            # è®¡ç®—æ–°çš„è¿‡æœŸæ—¶é—´
            if current_expires_at:
                new_expires_at = current_expires_at + datetime.timedelta(minutes=additional_minutes)
                
                # æ›´æ–°è¿‡æœŸæ—¶é—´
                cursor.execute(
                    """
                    UPDATE crawler_lock
                    SET expires_at = %s
                    WHERE lock_name = %s
                    """,
                    (new_expires_at, 'main_crawler_lock')
                )
                
                # åŒæ—¶æ›´æ–°ä»»åŠ¡çš„é¢„è®¡ç»“æŸæ—¶é—´
                cursor.execute(
                    """
                    UPDATE crawl_task
                    SET expected_end_time = %s
                    WHERE id = %s
                    """,
                    (new_expires_at, task_id)
                )
                
                # æäº¤äº‹åŠ¡
                conn.commit()
                logger.info(f"ä»»åŠ¡ {task_id} æˆåŠŸå»¶é•¿çˆ¬è™«é”è¶…æ—¶æ—¶é—´ {additional_minutes} åˆ†é’Ÿï¼Œæ–°çš„è¿‡æœŸæ—¶é—´: {new_expires_at}")
                return True
            else:
                logger.warning(f"ä»»åŠ¡ {task_id} çš„çˆ¬è™«é”æ²¡æœ‰è¿‡æœŸæ—¶é—´ï¼Œæ— æ³•å»¶é•¿")
                conn.rollback()
                return False
        else:
            logger.warning(f"ä»»åŠ¡ {task_id} å°è¯•å»¶é•¿ä¸å±äºå®ƒçš„é”")
            conn.rollback()
            return False
    except Exception as e:
        logger.error(f"å»¶é•¿çˆ¬è™«é”è¶…æ—¶æ—¶é—´å¤±è´¥: {str(e)}")
        if 'conn' in locals() and conn:
            conn.rollback()
        return False
    finally:
        if 'conn' in locals() and conn:
            conn.autocommit = True
            connection_pool.putconn(conn)

def is_crawler_locked():
    """
    æ£€æŸ¥çˆ¬è™«é”æ˜¯å¦è¢«å ç”¨
    
    Returns:
        tuple: (æ˜¯å¦é”å®š, é”å®šçš„ä»»åŠ¡ID) å¦‚æœæœªé”å®šåˆ™ä»»åŠ¡IDä¸ºNone
    """
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•æ£€æŸ¥çˆ¬è™«é”çŠ¶æ€")
        return (True, None)  # å¦‚æœæ— æ³•æ£€æŸ¥ï¼Œå‡è®¾å·²é”å®š
    
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        cursor.execute(
            "SELECT is_locked, locked_by, expires_at FROM crawler_lock WHERE lock_name = %s",
            ('main_crawler_lock',)
        )
        lock_info = cursor.fetchone()
        
        if not lock_info:
            logger.error("çˆ¬è™«é”è®°å½•ä¸å­˜åœ¨")
            return (True, None)
        
        is_locked, locked_by, expires_at = lock_info
        current_time = datetime.datetime.now()
        
        # æ£€æŸ¥é”æ˜¯å¦å·²è¿‡æœŸ
        if is_locked and expires_at and current_time > expires_at:
            logger.info("çˆ¬è™«é”å·²è¿‡æœŸï¼Œå°†è‡ªåŠ¨é‡Šæ”¾")
            
            # é‡Šæ”¾è¿‡æœŸçš„é”
            cursor.execute(
                """
                UPDATE crawler_lock
                SET is_locked = %s, locked_by = NULL, locked_at = NULL, expires_at = NULL
                WHERE lock_name = %s
                """,
                (False, 'main_crawler_lock')
            )
            conn.commit()
            
            return (False, None)
        
        return (is_locked, locked_by)
    except Exception as e:
        logger.error(f"æ£€æŸ¥çˆ¬è™«é”çŠ¶æ€å¤±è´¥: {str(e)}")
        return (True, None)  # å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œå‡è®¾å·²é”å®š
    finally:
        if 'conn' in locals() and conn:
            connection_pool.putconn(conn)

def queue_crawl_task(task_id):
    """
    å°†çˆ¬è™«ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­ç­‰å¾…æ‰§è¡Œ
    
    Args:
        task_id: çˆ¬è™«ä»»åŠ¡ID
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸæ·»åŠ åˆ°é˜Ÿåˆ—
    """
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—")
        return False
    
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        # è·å–å½“å‰æœ€å¤§é˜Ÿåˆ—ä½ç½®
        cursor.execute("SELECT MAX(queue_position) FROM crawl_task WHERE status = 'Queued'")
        max_position = cursor.fetchone()[0]
        
        if max_position is None:
            max_position = 0
        
        next_position = max_position + 1
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºé˜Ÿåˆ—ä¸­
        cursor.execute(
            """
            UPDATE crawl_task
            SET status = %s, queue_position = %s
            WHERE id = %s
            """,
            ('Queued', next_position, task_id)
        )
        
        conn.commit()
        logger.info(f"ä»»åŠ¡ {task_id} å·²æ·»åŠ åˆ°é˜Ÿåˆ—ï¼Œä½ç½® {next_position}")
        return True
    except Exception as e:
        logger.error(f"å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—å¤±è´¥: {str(e)}")
        return False
    finally:
        if 'conn' in locals() and conn:
            connection_pool.putconn(conn)

def start_next_queued_task():
    """
    å¯åŠ¨é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªçˆ¬è™«ä»»åŠ¡
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸå¯åŠ¨ä¸‹ä¸€ä¸ªä»»åŠ¡
    """
    if not connection_pool:
        logger.error("æ•°æ®åº“è¿æ¥æ± ä¸å¯ç”¨ï¼Œæ— æ³•å¯åŠ¨ä¸‹ä¸€ä¸ªä»»åŠ¡")
        return False
    
    try:
        # æ£€æŸ¥é”æ˜¯å¦å¯ç”¨
        is_locked, _ = is_crawler_locked()
        if is_locked:
            logger.info("çˆ¬è™«é”ä»åœ¨ä½¿ç”¨ä¸­ï¼Œæ— æ³•å¯åŠ¨ä¸‹ä¸€ä¸ªä»»åŠ¡")
            return False
        
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        # è·å–é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªä»»åŠ¡
        cursor.execute(
            """
            SELECT id, city, city_code
            FROM crawl_task
            WHERE status = 'Queued'
            ORDER BY queue_position ASC
            LIMIT 1
            """
        )
        next_task = cursor.fetchone()
        
        if not next_task:
            logger.info("é˜Ÿåˆ—ä¸­æ²¡æœ‰ç­‰å¾…çš„ä»»åŠ¡")
            return False
        
        task_id, city, city_code = next_task
        logger.info(f"å‡†å¤‡å¯åŠ¨é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªä»»åŠ¡: {task_id} - {city}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿›è¡Œä¸­
        cursor.execute(
            """
            UPDATE crawl_task
            SET status = %s, queue_position = 0
            WHERE id = %s
            """,
            ('In Progress', task_id)
        )
        conn.commit()
        
        # å¯åŠ¨çˆ¬è™«ä»»åŠ¡ï¼ˆå¼‚æ­¥æ–¹å¼ï¼‰
        import threading
        thread = threading.Thread(
            target=start_queued_crawler_task, 
            args=(task_id, city, city_code),
            daemon=True
        )
        thread.start()
        
        logger.info(f"å·²å¯åŠ¨é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªä»»åŠ¡: {task_id}")
        return True
    except Exception as e:
        logger.error(f"å¯åŠ¨ä¸‹ä¸€ä¸ªä»»åŠ¡å¤±è´¥: {str(e)}")
        return False
    finally:
        if 'conn' in locals() and conn:
            connection_pool.putconn(conn)

def start_queued_crawler_task(task_id, city, city_code, max_pages=5):
    """
    å¯åŠ¨é˜Ÿåˆ—ä¸­çš„çˆ¬è™«ä»»åŠ¡
    
    Args:
        task_id: çˆ¬è™«ä»»åŠ¡ID
        city: åŸå¸‚åç§°
        city_code: åŸå¸‚ä»£ç 
        max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œé˜Ÿåˆ—ä¸­çš„ä»»åŠ¡: {task_id} - {city}")
        
        # è·å–çˆ¬è™«é”
        if not acquire_crawler_lock(task_id, max_pages):
            logger.error(f"æ— æ³•è·å–çˆ¬è™«é”ï¼Œå–æ¶ˆæ‰§è¡Œä»»åŠ¡ {task_id}")
            return
        
        try:
            # æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
            crawl_city_with_selenium(city, city_code, max_pages, task_id)
            
            # ä»»åŠ¡å®Œæˆåï¼Œæ£€æŸ¥çŠ¶æ€å¹¶æ‰§è¡Œæ•°æ®åˆ†æ
            conn = connection_pool.getconn()
            cursor = conn.cursor()
            cursor.execute(
                "SELECT status FROM crawl_task WHERE id = %s",
                (task_id,)
            )
            task_status = cursor.fetchone()
            connection_pool.putconn(conn)
            
            if task_status and task_status[0] == "Completed":
                logger.info(f"é˜Ÿåˆ—ä»»åŠ¡ {task_id} å·²å®Œæˆï¼Œå¯åŠ¨æ•°æ®åˆ†æ...")
                # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œæ•°æ®åˆ†æï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
                analysis_thread = threading.Thread(
                    target=run_data_analysis,
                    args=(task_id, city, city_code),
                    daemon=True
                )
                analysis_thread.start()
                logger.info(f"æ•°æ®åˆ†æä»»åŠ¡å·²åœ¨åå°å¯åŠ¨")
        finally:
            # ç¡®ä¿ä»»åŠ¡å®Œæˆåé‡Šæ”¾é”
            release_crawler_lock(task_id)
            
    except Exception as e:
        logger.error(f"æ‰§è¡Œé˜Ÿåˆ—ä¸­çš„ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
        try:
            update_crawl_task(task_id, "Failed", error_message=f"æ‰§è¡Œé˜Ÿåˆ—ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
        except:
            pass
        
        # ç¡®ä¿æ— è®ºå¦‚ä½•éƒ½é‡Šæ”¾é”
        try:
            release_crawler_lock(task_id)
        except:
            pass

# æ·»åŠ ä¸€ä¸ªå¤„ç†å•ä¸ªæˆ¿æºçš„å‡½æ•°ï¼Œç”¨äºå¤šçº¿ç¨‹è°ƒç”¨
def process_single_house(house_item, city_code, task_id, index, total):
    """
    å¤„ç†å•ä¸ªæˆ¿æºé¡¹ç›®ï¼Œä¸ç›´æ¥ä¿å­˜æ•°æ®åº“
    
    Args:
        house_item: æˆ¿æºå…ƒç´ 
        city_code: åŸå¸‚ä»£ç 
        task_id: ä»»åŠ¡ID
        index: æˆ¿æºç´¢å¼•
        total: æ€»æˆ¿æºæ•°
    
    Returns:
        dict: å¤„ç†åçš„æˆ¿æºä¿¡æ¯ï¼Œå¤±è´¥åˆ™è¿”å›None
    """
    try:
        logger.info(f"å¼€å§‹å¤„ç†ç¬¬ {index+1}/{total} ä¸ªæˆ¿æº")
        # æå–æˆ¿æºä¿¡æ¯
        house_info = extract_house_info(house_item, city_code, task_id)
        
        if house_info:
            logger.info(f"æˆåŠŸæå–æˆ¿æºä¿¡æ¯: {house_info.get('title', 'æ— æ ‡é¢˜')}")
            
            # ç”Ÿæˆå”¯ä¸€çš„house_id (å¦‚æœURLä¸­åŒ…å«)
            if house_info.get('url'):
                house_id = get_house_id_from_url(house_info['url'])
                logger.info(f"ä»URLæå–çš„house_id: {house_id}")
                house_info['house_id'] = house_id
            else:
                logger.warning(f"æˆ¿æºä¿¡æ¯ä¸­ä¸åŒ…å«URL")
                # ç”Ÿæˆä¸€ä¸ªéšæœºID
                import uuid
                house_info['house_id'] = f"RAND{str(uuid.uuid4())[:20]}"
                logger.info(f"ç”Ÿæˆéšæœºhouse_id: {house_info['house_id']}")
            
            # è¿”å›å¤„ç†å¥½çš„æˆ¿æºä¿¡æ¯ï¼Œä¸ç«‹å³ä¿å­˜
            return house_info
        else:
            logger.warning(f"æˆ¿æºä¿¡æ¯æå–å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡¹")
            return None
    except Exception as item_err:
        logger.error(f"å¤„ç†ç¬¬ {index+1} ä¸ªæˆ¿æºæ—¶å‡ºé”™: {str(item_err)}")
        return None

# æ·»åŠ ä¸€ä¸ªå¤„ç†æ•´ä¸ªé¡µé¢æˆ¿æºçš„å‡½æ•°
def process_house_items(driver, task_id):
    """
    å¤„ç†é¡µé¢ä¸Šçš„æ‰€æœ‰æˆ¿æºé¡¹ç›®ï¼Œä½¿ç”¨æ‰¹é‡ä¿å­˜å‡å°‘æ•°æ®åº“è¿æ¥æ¶ˆè€—
    
    Args:
        driver: DrissionPageå¯¹è±¡
        task_id: ä»»åŠ¡ID
    
    Returns:
        tuple: (æˆåŠŸä¿å­˜çš„æˆ¿æºæ•°, æ€»æˆ¿æºæ•°)
    """
    try:
        # è·å–åŸå¸‚ä»£ç ï¼Œä»URLä¸­æå–
        current_url = driver.url
        city_code = current_url.split(".")[0].split("//")[1]
        logger.info(f"å½“å‰é¡µé¢URL: {current_url}, æå–åŸå¸‚ä»£ç : {city_code}")
        
        # è·å–æ‰€æœ‰æˆ¿æºå…ƒç´ 
        house_items = driver.eles('css:div.content__list--item')
        logger.info(f"åœ¨é¡µé¢ä¸Šæ‰¾åˆ° {len(house_items)} ä¸ªæˆ¿æºé¡¹ç›®")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æˆ¿æºä¿¡æ¯æå–
        max_workers = min(10, len(house_items))  # è®¾ç½®æœ€å¤§çº¿ç¨‹æ•°ï¼Œé¿å…åˆ›å»ºè¿‡å¤šçº¿ç¨‹
        logger.info(f"åˆ›å»ºçº¿ç¨‹æ± ï¼Œæœ€å¤§çº¿ç¨‹æ•°: {max_workers}")
        
        # åˆå§‹åŒ–æˆ¿æºä¿¡æ¯é›†åˆ
        valid_house_info_list = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨å¹¶è¡Œå¤„ç†æˆ¿æº
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æˆ¿æºå¤„ç†ä»»åŠ¡
            future_to_house = {
                executor.submit(
                    process_single_house, 
                    house_item, 
                    city_code, 
                    task_id, 
                    i, 
                    len(house_items)
                ): i for i, house_item in enumerate(house_items)
            }
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_house):
                house_index = future_to_house[future]
                try:
                    house_info = future.result()
                    if house_info:
                        # æ”¶é›†æœ‰æ•ˆçš„æˆ¿æºä¿¡æ¯
                        valid_house_info_list.append(house_info)
                except Exception as e:
                    logger.error(f"å¤„ç†æˆ¿æº {house_index+1} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        
        # å¯¹æ”¶é›†åˆ°çš„æˆ¿æºä¿¡æ¯è¿›è¡Œæ‰¹é‡ä¿å­˜
        logger.info(f"å¼€å§‹æ‰¹é‡ä¿å­˜ {len(valid_house_info_list)} ä¸ªæˆ¿æºä¿¡æ¯")
        
        if valid_house_info_list:
            # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
            save_result = batch_save_house_info(valid_house_info_list)
            success_count = save_result['success']
            failed_count = save_result['failed']
        else:
            success_count = 0
            failed_count = 0
        
        logger.info(f"é¡µé¢å¤„ç†å®Œæˆï¼ŒæˆåŠŸä¿å­˜: {success_count}, å¤±è´¥: {failed_count}, æ€»æ•°: {len(house_items)}")
        return success_count, len(house_items)
    
    except Exception as e:
        logger.error(f"å¤„ç†æˆ¿æºåˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return 0, 0

# åœ¨çˆ¬å–å®Œæˆåæ·»åŠ æ›´è¯¦ç»†çš„ç»Ÿè®¡ä»£ç ï¼Œæ”¾åœ¨process_house_itemsåé¢è°ƒç”¨
def get_crawl_statistics(task_id):
    """è·å–å½“å‰çˆ¬å–ä»»åŠ¡çš„ç»Ÿè®¡ä¿¡æ¯"""
    if not connection_pool:
        return {"task_items": 0, "total_items": 0, "success_pages": 0}
    
    conn = None
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        # ç»Ÿè®¡ä¸å½“å‰ä»»åŠ¡å…³è”çš„æˆ¿æºæ•°
        cursor.execute(
            "SELECT COUNT(*) FROM house_info WHERE task_id = %s",
            (task_id,)
        )
        task_items_count = cursor.fetchone()[0]
        
        # ç»Ÿè®¡æ€»æˆ¿æºæ•°
        cursor.execute("SELECT COUNT(*) FROM house_info")
        total_items_count = cursor.fetchone()[0]
        
        # ç»Ÿè®¡å½“å‰ä»»åŠ¡çš„æˆåŠŸé¡µé¢æ•°
        cursor.execute(
            "SELECT COUNT(*) FROM crawled_pages WHERE task_id = %s AND success = true",
            (task_id,)
        )
        success_pages_count = cursor.fetchone()[0]
        
        return {
            "task_items": task_items_count,
            "total_items": total_items_count,
            "success_pages": success_pages_count
        }
    except Exception as e:
        logger.error(f"è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return {"task_items": 0, "total_items": 0, "success_pages": 0}
    finally:
        # ç¡®ä¿åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æ­£ç¡®å½’è¿˜è¿æ¥
        if conn:
            try:
                connection_pool.putconn(conn)
            except Exception as conn_err:
                logger.error(f"å½’è¿˜æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {str(conn_err)}")
                # è¿æ¥å½’è¿˜é”™è¯¯ä¸åº”å½±å“å‡½æ•°è¿”å›

@with_db_connection
def batch_save_house_info(conn, house_info_list):
    """æ‰¹é‡ä¿å­˜å¤šä¸ªæˆ¿æºä¿¡æ¯åˆ°æ•°æ®åº“ï¼Œå‡å°‘æ•°æ®åº“è¿æ¥æ¬¡æ•°
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        house_info_list: æˆ¿æºä¿¡æ¯åˆ—è¡¨
        
    Returns:
        dict: åŒ…å«æˆåŠŸå’Œå¤±è´¥çš„æˆ¿æºæ•°é‡
    """
    if not house_info_list:
        logger.warning("æ²¡æœ‰æˆ¿æºä¿¡æ¯å¯ä¿å­˜")
        return {"success": 0, "failed": 0}
    
    try:
        cursor = conn.cursor()
        success_count = 0
        failed_count = 0
        
        # å¼€å§‹äº‹åŠ¡
        try:
            conn.autocommit = False
        except Exception as e:
            logger.warning(f"è®¾ç½®autocommit=Falseå¤±è´¥: {str(e)}ï¼Œå°†å°è¯•ç»§ç»­æ‰§è¡Œ")
        
        # ä¸ºæ¯ä¸ªæˆ¿æºç”Ÿæˆhouse_idå¹¶å°è¯•ä¿å­˜
        for house_info in house_info_list:
            try:
                # ç¡®ä¿æœ‰house_id
                if 'house_id' not in house_info or not house_info['house_id']:
                    house_info['house_id'] = get_house_id_from_url(house_info['url'])
                    if not house_info['house_id']:
                        import hashlib
                        house_info['house_id'] = "HASH" + hashlib.md5(house_info['url'].encode()).hexdigest()[:20]
                
                # è§£ææˆ·å‹å­—æ®µï¼Œæå–æˆ¿é—´ã€å…å’Œå«ç”Ÿé—´æ•°é‡
                layout_str = house_info.get('layout', '')
                room_description, room_count, hall_count, bath_count = parse_layout_to_components(layout_str)
                
                # è®¡ç®—å•ä»·
                unit_price = 0
                try:
                    price = float(house_info.get('price', 0))
                    area = float(house_info.get('area', 0))
                    if price > 0 and area > 0:
                        unit_price = round(price / area, 2)
                except (ValueError, TypeError):
                    logger.warning(f"è®¡ç®—å•ä»·å¤±è´¥: price={house_info.get('price')}, area={house_info.get('area')}")
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„house_id
                cursor.execute(
                    "SELECT id FROM house_info WHERE house_id = %s",
                    (house_info['house_id'],)
                )
                existing_by_id = cursor.fetchone()
                
                if existing_by_id:
                    # æ›´æ–°å·²å­˜åœ¨çš„è®°å½•
                    cursor.execute("""
                    UPDATE house_info SET
                        title = %s,
                        price = %s,
                        layout = %s,
                        size = %s,
                        floor = %s,
                        direction = %s,
                        subway = %s,
                        location_qu = %s,
                        location_big = %s,
                        publish_date = %s,
                        features = %s,
                        image = %s,
                        link = %s,
                        city_code = %s,
                        room = %s,
                        room_count = %s,
                        hall_count = %s,
                        bath_count = %s,
                        unit_price = %s,
                        last_updated = %s
                    WHERE house_id = %s
                    """, (
                        house_info['title'],
                        house_info['price'],
                        house_info['layout'],
                        house_info['area'],
                        house_info['floor'],
                        house_info['direction'],
                        house_info['subway'],
                        house_info['district'],
                        house_info['community'],
                        house_info['publish_date'],
                        json.dumps(house_info['features'], ensure_ascii=False),
                        house_info.get('image_url', ''),
                        house_info['url'],
                        house_info['city_code'],
                        room_description,
                        room_count,
                        hall_count,
                        bath_count,
                        unit_price,
                        datetime.datetime.now(),
                        house_info['house_id']
                    ))
                    success_count += 1
                else:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„URLå’ŒåŸå¸‚ä»£ç 
                    cursor.execute(
                        "SELECT id FROM house_info WHERE link = %s AND city_code = %s",
                        (house_info['url'], house_info['city_code'])
                    )
                    existing_by_url = cursor.fetchone()
                    
                    if existing_by_url:
                        # æ›´æ–°å·²å­˜åœ¨è®°å½•
                        cursor.execute("""
                        UPDATE house_info SET
                            title = %s,
                            price = %s,
                            layout = %s,
                            size = %s,
                            floor = %s,
                            direction = %s,
                            subway = %s,
                            location_qu = %s,
                            location_big = %s,
                            publish_date = %s,
                            features = %s,
                            image = %s,
                            house_id = %s,
                            room = %s,
                            room_count = %s,
                            hall_count = %s,
                            bath_count = %s,
                            unit_price = %s,
                            last_updated = %s
                        WHERE link = %s AND city_code = %s
                        """, (
                            house_info['title'],
                            house_info['price'],
                            house_info['layout'],
                            house_info['area'],
                            house_info['floor'],
                            house_info['direction'],
                            house_info['subway'],
                            house_info['district'],
                            house_info['community'],
                            house_info['publish_date'],
                            json.dumps(house_info['features'], ensure_ascii=False),
                            house_info.get('image_url', ''),
                            house_info['house_id'],
                            room_description,
                            room_count,
                            hall_count,
                            bath_count,
                            unit_price,
                            datetime.datetime.now(),
                            house_info['url'],
                            house_info['city_code']
                        ))
                        success_count += 1
                    else:
                        # æ’å…¥æ–°è®°å½•
                        cursor.execute("""
                        INSERT INTO house_info (
                            link, title, price, layout, size, floor, direction,
                            subway, location_qu, location_big, city_code, publish_date, 
                            features, image, created_at, last_updated, task_id, house_id,
                            room, room_count, hall_count, bath_count, unit_price
                        ) VALUES (
                            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                            %s, %s, %s, %s, %s
                        )
                        """, (
                            house_info['url'],
                            house_info['title'],
                            house_info['price'],
                            house_info['layout'],
                            house_info['area'],
                            house_info['floor'],
                            house_info['direction'],
                            house_info['subway'],
                            house_info['district'],
                            house_info['community'],
                            house_info['city_code'],
                            house_info['publish_date'],
                            json.dumps(house_info['features'], ensure_ascii=False),
                            house_info.get('image_url', ''),
                            datetime.datetime.now(),
                            datetime.datetime.now(),
                            house_info.get('task_id'),
                            house_info['house_id'],
                            room_description,
                            room_count,
                            hall_count,
                            bath_count,
                            unit_price
                        ))
                        success_count += 1
            except Exception as e:
                logger.error(f"ä¿å­˜æˆ¿æº {house_info.get('house_id', 'æœªçŸ¥')} å¤±è´¥: {str(e)}")
                failed_count += 1
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæˆ¿æºï¼Œè€Œä¸æ˜¯æ•´ä½“å¤±è´¥
                continue
        
        # æ‰€æœ‰æ“ä½œæˆåŠŸï¼Œæäº¤äº‹åŠ¡
        try:
            conn.commit()
            logger.info(f"æ‰¹é‡ä¿å­˜æˆ¿æºå®Œæˆï¼ŒæˆåŠŸ: {success_count}ï¼Œå¤±è´¥: {failed_count}")
        except Exception as commit_err:
            logger.error(f"æäº¤äº‹åŠ¡å¤±è´¥: {str(commit_err)}")
            try:
                conn.rollback()
            except:
                pass
            return {"success": 0, "failed": len(house_info_list)}
            
        return {"success": success_count, "failed": failed_count}
    
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¿å­˜æˆ¿æºå¤±è´¥: {str(e)}")
        try:
            conn.rollback()
        except Exception as rollback_err:
            logger.error(f"å›æ»šäº‹åŠ¡å¤±è´¥: {str(rollback_err)}")
        return {"success": 0, "failed": len(house_info_list)}
    finally:
        # ç¡®ä¿æ¢å¤autocommitçŠ¶æ€
        try:
            conn.autocommit = True
        except Exception as ac_err:
            logger.warning(f"æ¢å¤autocommitçŠ¶æ€å¤±è´¥: {str(ac_err)}")

def run_data_analysis(task_id, city, city_code):
    """
    åœ¨çˆ¬è™«ä»»åŠ¡å®Œæˆåæ‰§è¡Œæ•°æ®åˆ†æ
    
    Args:
        task_id: çˆ¬è™«ä»»åŠ¡ID
        city: åŸå¸‚åç§°
        city_code: åŸå¸‚ä»£ç 
    """
    try:
        logger.info(f"çˆ¬è™«ä»»åŠ¡ {task_id} å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œæ•°æ®åˆ†æ...")
        
        # åˆ›å»ºRentalDataProcessorå®ä¾‹
        processor = data_processor.RentalDataProcessor()
        
        try:
            # åŠ è½½å½“å‰ä»»åŠ¡çš„æ•°æ®
            logger.info(f"æ­£åœ¨åŠ è½½ä»»åŠ¡ {task_id} çš„æ•°æ®...")
            df = processor.load_data_from_db(city=city, task_id=task_id)
            
            if df:
                # æ‰§è¡Œæ‰€æœ‰åˆ†æ
                logger.info(f"å¼€å§‹ä¸ºåŸå¸‚ {city} æ‰§è¡Œæ•°æ®åˆ†æ...")
                processor.run_all_analysis(df, city=city)
                logger.info(f"æ•°æ®åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“")
            else:
                logger.warning(f"æœªæ‰¾åˆ°ä»»åŠ¡ {task_id} çš„æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡åˆ†æ")
        finally:
            # ç¡®ä¿å…³é—­Sparkä¼šè¯
            processor.close()
            
    except Exception as e:
        logger.error(f"æ‰§è¡Œæ•°æ®åˆ†ææ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œåˆ™åˆå§‹åŒ–éªŒè¯ç®¡ç†å™¨
if __name__ == '__main__':
    # åˆå§‹åŒ–éªŒè¯ç®¡ç†å™¨
    verification_manager.init()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä»»åŠ¡
    try:
        conn = connection_pool.getconn()
        cursor = conn.cursor()
        
        cursor.execute(
            """
            SELECT id, city, city_code, start_time, status 
            FROM crawl_task 
            WHERE status IN ('In Progress', 'Incomplete', 'Queued') 
            ORDER BY start_time DESC
            LIMIT 5
            """
        )
        unfinished_tasks = cursor.fetchall()
        
        # æ£€æŸ¥çˆ¬è™«é”çŠ¶æ€
        is_locked, locked_task_id = is_crawler_locked()
        if is_locked:
            print("\nâš ï¸ è­¦å‘Š: çˆ¬è™«æ­£åœ¨è¿è¡Œä¸­ï¼Œä»»åŠ¡ID:", locked_task_id)
            print("æ–°ä»»åŠ¡å°†è¢«æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­ç­‰å¾…æ‰§è¡Œ")
        
        if unfinished_tasks:
            print("\n===== æ£€æµ‹åˆ°æœªå®Œæˆçš„çˆ¬å–ä»»åŠ¡ =====")
            print("ID\tåŸå¸‚\tåŸå¸‚ä»£ç \tå¼€å§‹æ—¶é—´\t\t\tçŠ¶æ€")
            print("-" * 70)
            
            for task in unfinished_tasks:
                task_id, city, city_code, start_time, status = task
                print(f"{task_id}\t{city}\t{city_code}\t\t{start_time}\t{status}")
            
            print("-" * 70)
            continue_task = input("æ˜¯å¦è¦ç»§ç»­ä¸€ä¸ªæœªå®Œæˆçš„ä»»åŠ¡ï¼Ÿ(y/nï¼Œé»˜è®¤ä¸ºn): ").strip().lower()
            
            if continue_task == 'y':
                task_id = input("è¯·è¾“å…¥è¦ç»§ç»­çš„ä»»åŠ¡ID: ")
                try:
                    task_id = int(task_id)
                    
                    # è·å–ä»»åŠ¡ä¿¡æ¯
                    cursor.execute(
                        "SELECT city, city_code, status FROM crawl_task WHERE id = %s",
                        (task_id,)
                    )
                    task_info = cursor.fetchone()
                    
                    if task_info:
                        city, city_code, status = task_info
                        print(f"å°†ç»§ç»­çˆ¬å–ä»»åŠ¡ ID: {task_id}, åŸå¸‚: {city}({city_code})")
                        
                        # è·å–å·²çˆ¬å–é¡µæ•°
                        crawled_pages = get_crawled_pages(task_id)
                        print(f"å·²æˆåŠŸçˆ¬å– {len(crawled_pages)} é¡µ")
                        
                        max_pages = input(f"è¯·è¾“å…¥è¦çˆ¬å–çš„æœ€å¤§é¡µæ•° (å·²çˆ¬å–é¡µé¢ä¼šè‡ªåŠ¨è·³è¿‡ï¼Œé»˜è®¤: 5): ")
                        try:
                            max_pages = int(max_pages)
                        except:
                            max_pages = 5
                        
                        print(f"ç»§ç»­çˆ¬å– {city}({city_code}) çš„ç§Ÿæˆ¿ä¿¡æ¯ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")
                        
                        # æ£€æŸ¥çˆ¬è™«é”çŠ¶æ€
                        is_locked, locked_task_id = is_crawler_locked()
                        if is_locked and locked_task_id != task_id:
                            print(f"\nâš ï¸ çˆ¬è™«æ­£åœ¨è¿è¡Œä¸­ï¼Œä»»åŠ¡å°†è¢«æ·»åŠ åˆ°é˜Ÿåˆ—ç­‰å¾…æ‰§è¡Œ")
                            update_crawl_task(task_id, "Queued")
                            queue_crawl_task(task_id)
                            print(f"ä»»åŠ¡ {task_id} å·²æ·»åŠ åˆ°é˜Ÿåˆ—ï¼Œå½“å‰çˆ¬è™«å®Œæˆåå°†è‡ªåŠ¨æ‰§è¡Œ")
                        else:
                            # å¼€å§‹çˆ¬å–
                            if is_locked and locked_task_id == task_id:
                                print(f"å½“å‰ä»»åŠ¡å·²ç»åœ¨æ‰§è¡Œä¸­")
                            else:
                                # è·å–çˆ¬è™«é”
                                if acquire_crawler_lock(task_id, max_pages):
                                    try:
                                        # å¼€å§‹çˆ¬å–
                                        task_id = crawl_city_with_selenium(city, city_code, max_pages, task_id)
                                        
                                        # å¯¼å‡ºæ•°æ®
                                        if task_id:
                                            houses, _ = extract_house_info(None, task_id)
                                            export_to_csv(houses, f"{city}_rental_data_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.csv")
                                        
                                        # ç¨‹åºç»“æŸ
                                        print("çˆ¬å–ä»»åŠ¡å®Œæˆ")
                                        
                                        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼Œå¦‚æœæˆåŠŸå®Œæˆåˆ™è¿è¡Œæ•°æ®åˆ†æ
                                        conn = connection_pool.getconn()
                                        cursor = conn.cursor()
                                        cursor.execute(
                                            "SELECT status FROM crawl_task WHERE id = %s",
                                            (task_id,)
                                        )
                                        task_status = cursor.fetchone()
                                        connection_pool.putconn(conn)
                                        
                                        if task_status and task_status[0] == "Completed":
                                            print("çˆ¬è™«ä»»åŠ¡å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œæ•°æ®åˆ†æ...")
                                            # ç›´æ¥è°ƒç”¨åˆ†æå‡½æ•°ï¼Œè€Œä¸æ˜¯å¼€æ–°çº¿ç¨‹ï¼ˆå› ä¸ºæ˜¯ä¸»ç¨‹åºçš„æœ€åé˜¶æ®µï¼‰
                                            run_data_analysis(task_id, city, city_code)
                                            print("æ•°æ®åˆ†æå®Œæˆ")
                                    finally:
                                        # ç¡®ä¿é‡Šæ”¾é”
                                        release_crawler_lock(task_id)
                                else:
                                    print("æ— æ³•è·å–çˆ¬è™«é”ï¼Œå¯èƒ½å¦ä¸€ä¸ªçˆ¬è™«æ­£åœ¨è¿è¡Œ")
                                    # å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—
                                    update_crawl_task(task_id, "Queued")
                                    queue_crawl_task(task_id)
                                    print(f"ä»»åŠ¡ {task_id} å·²æ·»åŠ åˆ°é˜Ÿåˆ—ï¼Œå½“å‰çˆ¬è™«å®Œæˆåå°†è‡ªåŠ¨æ‰§è¡Œ")
                        
                        exit(0)
                    else:
                        print(f"æœªæ‰¾åˆ°ä»»åŠ¡ ID: {task_id}")
                except ValueError:
                    print("æ— æ•ˆçš„ä»»åŠ¡IDï¼Œè¯·è¾“å…¥æ•°å­—")
                except Exception as e:
                    print(f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
    except Exception as e:
        print(f"æŸ¥è¯¢æœªå®Œæˆä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
    finally:
        if 'conn' in locals() and conn:
            connection_pool.putconn(conn)
    
    # è·å–æ”¯æŒçš„åŸå¸‚åˆ—è¡¨
    cities = get_supported_cities()
    print("\næ”¯æŒçš„åŸå¸‚åˆ—è¡¨:")
    for city, code in cities.items():
        print(f"{city}: {code}")
    
    # é€‰æ‹©è¦çˆ¬å–çš„åŸå¸‚
    city_name = input("\nè¯·è¾“å…¥è¦çˆ¬å–çš„åŸå¸‚åç§° (é»˜è®¤: æ¹›æ±Ÿ): ") or "æ¹›æ±Ÿ"
    city_code = cities.get(city_name)
    
    if not city_code:
        print(f"ä¸æ”¯æŒçš„åŸå¸‚: {city_name}ï¼Œå°†ä½¿ç”¨é»˜è®¤åŸå¸‚ä»£ç : zhanjiang")
        city_code = "zhanjiang"
    
    # è®¾ç½®çˆ¬å–é¡µæ•°
    max_pages = input("è¯·è¾“å…¥è¦çˆ¬å–çš„æœ€å¤§é¡µæ•° (é»˜è®¤: 5): ")
    try:
        max_pages = int(max_pages)
    except:
        max_pages = 5
    
    print(f"å¼€å§‹çˆ¬å– {city_name}({city_code}) çš„ç§Ÿæˆ¿ä¿¡æ¯ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")
    print("è¿™å°†ä½¿ç”¨DrissionPageï¼Œè¯·ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨å’ŒWebDriver")
    print("å¦‚æœé‡åˆ°äººæœºéªŒè¯ï¼Œç¨‹åºä¼šè‡ªåŠ¨åˆ›å»ºéªŒè¯ä¼šè¯ï¼Œè¯·æŸ¥çœ‹æ§åˆ¶å°è·å–éªŒè¯URL")
    
    # åˆ›å»ºæ–°çš„çˆ¬å–ä»»åŠ¡
    task_id = start_crawl_task(city_name, city_code)
    
    if task_id:
        # æ£€æŸ¥çˆ¬è™«é”çŠ¶æ€
        is_locked, locked_task_id = is_crawler_locked()
        if is_locked:
            print(f"\nâš ï¸ çˆ¬è™«æ­£åœ¨è¿è¡Œä¸­ï¼Œä»»åŠ¡å°†è¢«æ·»åŠ åˆ°é˜Ÿåˆ—ç­‰å¾…æ‰§è¡Œ")
            update_crawl_task(task_id, "Queued")
            queue_crawl_task(task_id)
            print(f"ä»»åŠ¡ {task_id} å·²æ·»åŠ åˆ°é˜Ÿåˆ—ï¼Œå½“å‰çˆ¬è™«å®Œæˆåå°†è‡ªåŠ¨æ‰§è¡Œ")
        else:
            # è·å–çˆ¬è™«é”
            if acquire_crawler_lock(task_id, max_pages):
                try:
                    # å¼€å§‹çˆ¬å–
                    task_id = crawl_city_with_selenium(city_name, city_code, max_pages, task_id)
                    
                    # å¯¼å‡ºæ•°æ®
                    if task_id:
                        houses, _ = extract_house_info(None, task_id)
                        export_to_csv(houses, f"{city_name}_rental_data_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.csv")
                        
                        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼Œå¦‚æœæˆåŠŸå®Œæˆåˆ™è¿è¡Œæ•°æ®åˆ†æ
                        conn = connection_pool.getconn()
                        cursor = conn.cursor()
                        cursor.execute(
                            "SELECT status FROM crawl_task WHERE id = %s",
                            (task_id,)
                        )
                        task_status = cursor.fetchone()
                        connection_pool.putconn(conn)
                        
                        if task_status and task_status[0] == "Completed":
                            print("çˆ¬è™«ä»»åŠ¡å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œæ•°æ®åˆ†æ...")
                            # ç›´æ¥è°ƒç”¨åˆ†æå‡½æ•°ï¼Œè€Œä¸æ˜¯å¼€æ–°çº¿ç¨‹ï¼ˆå› ä¸ºæ˜¯ä¸»ç¨‹åºçš„æœ€åé˜¶æ®µï¼‰
                            run_data_analysis(task_id, city_name, city_code)
                            print("æ•°æ®åˆ†æå®Œæˆ")
                finally:
                    # ç¡®ä¿é‡Šæ”¾é”
                    release_crawler_lock(task_id)